{
<<<<<<< HEAD
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqeaYfHXGAt8"
      },
      "source": [
        "# Coursework 1 - Mathematics for Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjnXLCaKGAt9"
      },
      "source": [
        "## CID: 01877978\n",
        "\n",
        "**Colab link:** insert colab link here\n",
        "\n",
        "***\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d81KQmqIGAt9"
      },
      "source": [
        "## Part 1: Quickfire questions [3 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSUkzJb3GAt9"
      },
      "source": [
        "#### Question 1 (True risk / Empirical risk):\n",
        "\n",
        "True risk $R(f) = \\mathbb{E}_D[L(f(\\bold{x},\\bold{y}))]$\\\n",
        "Empirical risk $\\hat{R}(f) = \\frac{1}{N}\\sum_{i=1}^N L(f(\\bold{x}^i),\\bold{y}^i)$\n",
        "\n",
        "The True risk $R(f)$ can only be evaluated by knowing the joint distrubution of ($\\bold{x}, \\bold{y})$, so is instead approximated by using realisations of the distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suNH_xDLGAt9"
      },
      "source": [
        "#### Question 2 ('Large' or 'rich' hypothesis class):\n",
        "Hypothesis class $F$ is the set of functions which can be used to model $(\\bold{x}, f(\\bold{x}))$\\\n",
        "Downfalls are it is harder to optimize over as you have introduced more parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMx6oUl0GAt9"
      },
      "source": [
        "#### Question 3 (Dataset splitting):\n",
        "\n",
        "No, you chose the model with the maximum validation set score.\n",
        "\n",
        "If all the model validation set score was i.i.d $\\sim N(\\mu, \\sigma^2)$,the maximum would be distributed with $ P(X_{max} \\leq x) = \\prod_{i=1}^{N}[P(X_i \\leq x)] = \\Phi^n(x)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz_reCJlGAt9"
      },
      "source": [
        "#### Question 4 (Occamâ€™s razor):\n",
        "\n",
        "Among competing hypotheses that explain known observations equally well, we should choose the simplest one. In relation to data such as images, this could include"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArGqpktYGAt9"
      },
      "source": [
        "#### Question 5 (Generalisation error):\n",
        "\n",
        "You would expect a small generalisation error on a good model, i.e. it is not overfit on the training set so it performly similarly in training as in testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiDt-rbNGAt-"
      },
      "source": [
        "#### Question 6 (Rademacher complexity pt1):\n",
        "\n",
        "High empirical rademacher complexity means for a given sample $\\{ z_1, z_2, ... z_m\\}$, that our function class does not contain a function that maps the input well to random noise. This means our function class is not too wide that it could fit to data which is unpredictable (random noise)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9CHk3zXGAt-"
      },
      "source": [
        "#### Question 7 (Rademacher complexity pt2):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5APbH5wGAt-"
      },
      "source": [
        "#### Question 8 (Regularisation term in the loss function):\n",
        "\n",
        "Regularisation term in the loss function can encourage the model to adopt a simpler complexity in the training. This can lead to a model with better generalisation ability and which is not overfit to the training data. Also"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHZRJk_0GAt-"
      },
      "source": [
        "#### Question 9 (Momentum gradient descent):\n",
        "\n",
        "Momentum helps the gradient descent escape local minima pits (the momentum accumulated when going in to the local minima helps carry it out). It doesn't take very small steps at gentle gradients, the descent is generally smoother (the gradient of step taken is correlated to gradient of past step taken)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O3G7t7GGAt-"
      },
      "source": [
        "#### Question 10 (Adam):\n",
        "\n",
        "\\begin{align}\n",
        "&t &&= t + 1\\\\\n",
        "&m_t &&= \\beta_1 m_t + (1 - \\beta_1) \\left(\\frac{\\delta L}{\\delta w_t}\\right)\\\\\n",
        "&v_t &&= \\beta_2 v_t + (1 - \\beta_2) \\left(\\frac{\\delta L}{\\delta w_t}\\right)^2\\\\\n",
        "&\\hat{m}_t &&= m_t / (1 - \\beta_1^t)\\\\\n",
        "&\\hat{v}_t &&= v_t / (1 - \\beta_2^t)\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0ejxFM1GAt-"
      },
      "source": [
        "#### Question 11 (AdaGrad):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UXJJoI_GAt-"
      },
      "source": [
        "#### Question 12 (Decaying Learning Rate):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIqJTm9XGAt-"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Part 2: Short-ish proofs [6 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DoSY99ZGAt-"
      },
      "source": [
        "\n",
        "### Question 2.1: Bounds on the risk [1 point]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeL6-0HxGAt-"
      },
      "source": [
        "Hoeffdings inequality\n",
        "\n",
        "Let $X_1, X_2, X_3,..,X_m$ independent, taking values in $[a_i, b_i]$. With $S_m = \\sum_{i=1}^m X_i$.\\\n",
        "$\\forall \\epsilon > 0$\n",
        "\n",
        "$P(S_n - E[S_n] \\geq \\epsilon) \\leq exp\\left \\{ \\begin{aligned}\n",
        "    -\\frac{2\\epsilon^2}{\\sum_{i=1}^n(b_i - a_i)^2}\n",
        "    \\end{aligned}\n",
        "    \\right\\}$\n",
        "\n",
        "Viewing $L(f(x_i), y_i)$ i.i.d random variables (a function of the join distribution $(\\bold{x}, \\bold{y})$)\\\n",
        "Noting $0 \\leq L(f(x_i), y_i) \\leq 1, \\forall \\omega \\in \\Omega$, we take $b_i = 1, a_i = 0, \\forall i$\\\n",
        "and as they are i.i.d, $\\mathbb{E}[S_n] = \\mathbb{E}[L(f(\\bold{x}), \\bold{x})]$\n",
        "\n",
        " $\\forall \\epsilon$, we have\n",
        "$P(\\frac{1}{N}S_n - \\frac{1}{N} E[S_n] \\geq \\epsilon)  = p(S_n - E[S_n] \\geq N \\epsilon) \\leq exp\\left \\{ \\begin{aligned}\n",
        "    -\\frac{2(N \\epsilon)^2}{\\sum_{i=1}^N1^2}\n",
        "    \\end{aligned}\n",
        "    \\right\\}\n",
        "=exp\\left \\{ \\begin{aligned}\n",
        "    -2 N \\epsilon^2\n",
        "    \\end{aligned}\n",
        "    \\right\\}$\n",
        "\n",
        "\n",
        "3\\) d\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZVxO4y_GAt-"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.2: On semi-definiteness [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCSC6v2nGAt-"
      },
      "source": [
        "$f(y) \\geq f(x) + \\nabla f(x)^T(y-x)$\n",
        "\n",
        "Let $g(t) = f(x+tv)$\n",
        "\n",
        "$g'(t) = v^T \\, \\nabla f(x + vt)$\n",
        "\n",
        "$g''(t) = v^T \\cdot\\nabla^2 f(x + tv) \\cdot v$\n",
        "\n",
        "Showing g is convex.\\\n",
        "$\\forall t_1, t_2 \\in \\mathbb{R},\\alpha \\in (0,1),\\\\\n",
        "g(\\alpha t_1  + (1 - \\alpha)t_2) = f(\\bold{x} + (\\alpha t_1  + (1 - \\alpha)t_2)\\bold{v}) = f(\\alpha [\\bold{x} +  t_1 \\bold{v}] + (1-\\alpha)[ \\bold{x} + t_2 \\bold{v}]) \\leq \\alpha f(\\bold{x} +  t_1 \\bold{v}) + (1-\\alpha) f(\\bold{x} + t_2 \\bold{v})\\\\\n",
        "=\\alpha g(t_1) + (1 - \\alpha) g(t_2)$\n",
        "\n",
        "$g$ is convex $\\implies$ $g''(t) \\geq 0$\n",
        "\n",
        "As our choice of $\\bold{v}$ was arbitrary we have $\\forall \\bold{v} \\in \\mathbb{R^d}$, $0 \\leq g''(t) = \\bold{v} ^T \\, \\nabla ^2 f(x + tv) \\, \\bold{v}$\n",
        "\n",
        "So f is positive semi-definite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4EYvsuCGAt-"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.3: A quick recap of momentum [1 point]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWK98-yGGAt-"
      },
      "source": [
        "\n",
        "$\\bold{1)}$\n",
        "\n",
        "$\\forall i \\in \\{ 1, 2, 3,..,d\\}, \\, Q^T(e_i - x*), $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDGV3f_wGAt-"
      },
      "source": [
        "$\\bold{2)}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS7euK6fGAt_"
      },
      "source": [
        "$\\bold{3)}$\\\n",
        "Assuming it is diagnoalizable, the convergence goes at the rate of the greatest magnitude of eigenvalue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRurGEgpGAt_"
      },
      "source": [
        "$\\bold{4)}$\n",
        "\n",
        "$(\\beta + 1 - \\alpha \\lambda_i)^2 - 4 \\beta < 0 \\Longleftrightarrow $ R has complex eigenvalues, R having complex eigenvalues implies they have fixed magnitude $\\sqrt{\\beta}$ which is the rate of convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMeaP9IcGAt_"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.4: Convergence proof [3 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAXuTvKpGAt_"
      },
      "source": [
        "$\\bold{1)}$\n",
        "\n",
        "Iterative step: $x_{k+1} = x_k  - (\\nabla ^2(f(x_k))^{-1} \\nabla f(x_k)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-kerUk8GAt_"
      },
      "source": [
        "$\\bold{2)}$\n",
        "\n",
        "With $f(x) = \\frac{1}{2} x^t Q x + b^T x + c$\n",
        "\n",
        "$\\nabla f(x) = Qx + b$\\\n",
        "$\\nabla^2 f(x) = Q$\n",
        "\n",
        "so $x_{k+1} = X_k - Q^{-1} (Qx_k + b) = x_k - x_k + Q^{-1}b = - Q^{-1}b$\n",
        "\n",
        "$- Q^{-1}b = x$, satisfies $x = x - Q^{-1}(Qx + b)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7dgKPINGAt_"
      },
      "source": [
        "$\\bold{3)}$\\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHhAFz1MGAt_"
      },
      "source": [
        "$\\bold{4)}$\n",
        "\n",
        "$\\| x_1 - x^* \\|$, subbing in the definition of $x_1$\n",
        "\n",
        "$= \\| x_0 - (\\nabla ^2f(x_0))^{-1}(\\nabla f(x_0)) - x^* \\|$\n",
        "\n",
        "$ =\\| (\\nabla ^2f(x_0))^{-1} (\\nabla ^2f(x_0)) \\left[x_0 - (\\nabla ^2f(x_0))^{-1}(\\nabla f(x_0)) - x^* \\right]\\|$\n",
        "\n",
        "$ \\leq \\| (\\nabla ^2f(x_0))^{-1} \\| \\| (\\nabla ^2f(x_0)) (x_0 - x^*) - (\\nabla f(x_0)) \\|\\hspace{10pt}$ Using Lemma 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ3YgWzfGAt_"
      },
      "source": [
        "$\\bold{5)}$\\\n",
        "Choose $\\epsilon > 0 \\, : \\, (\\nabla^2 f(x_0))^{-1}$ exists\n",
        "\n",
        "Taylor series on $\\nabla f(x^*)$ around $x_0$\\\n",
        "$\\exists c \\in (x^*, x_0) \\, : \\, $\\\n",
        "$0 = \\nabla f(x^*) = \\nabla f(x_0) + \\nabla^2 f(x_0)(x^* - x_0) + \\frac{1}{2} (x^* - x_0)^T \\nabla^3 f(c) (x^* - x_0)$\n",
        "\n",
        "$\\hspace{10.5pt} \\nabla ^2f(x_0) (x_0 - x^*) - \\nabla f(x_0)\\\\\n",
        "= \\nabla ^2f(x_0) (x_0 - x^*) - \\nabla f(x_0) + \\nabla f(x^*)\\\\\n",
        "= \\nabla ^2f(x_0) (x_0 - x^*) - \\nabla f(x_0) + \\nabla f(x_0) + \\nabla^2 f(x_0)(x^* - x_0) + \\frac{1}{2} (x^* - x_0)^T \\nabla^3 f(c) (x^* - x_0)\\\\\n",
        "= \\nabla ^2f(x_0) (x_0 - x^*) - \\nabla f(x_0) + \\nabla f(x_0) - \\nabla^2 f(x_0)(x_0 - x^*) + \\frac{1}{2} (x^* - x_0)^T \\nabla^3 f(c) (x^* - x_0)\\\\\n",
        "= \\frac{1}{2} (x^* - x_0)^T \\nabla^3 f(x_0) (x^* - x_0)$\n",
        "\n",
        "$\\implies \\| \\nabla ^2f(x_0) (x_0 - x^*) - \\nabla f(x_0) \\| \\leq \\|(x^* - x_0)^T \\nabla^3 f(c) (x^* - x_0) \\|$\n",
        "\n",
        "$\\| x_1 - x^* \\|\n",
        "\\leq \\| (\\nabla ^2f(x_0))^{-1} \\| \\| (\\nabla ^2f(x_0)) (x_0 - x^*) - (\\nabla f(x_0)) \\|\n",
        "\\leq \\| (\\nabla ^2f(x_0))^{-1} \\| \\| \\nabla^3 f(c) \\| \\| (x_0 - x^*) \\| ^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqrnNl9qGAt_"
      },
      "source": [
        "$\\bold{ 6)}$\n",
        "\n",
        "Given $x_0 \\in B(x^*, \\epsilon)\\,  : \\, \\|x_0 - x^*\\| \\leq \\frac{\\alpha}{c_1 c_2}$\n",
        "\n",
        "Show $x_1 \\in B(x^*, \\epsilon) \\land \\|x_1 - x^* \\| \\leq \\frac{\\alpha}{c_1 c_2}$\n",
        "\n",
        "\n",
        "\n",
        "From 5\\)\\\n",
        "$x_0 \\in B(x^*, \\epsilon) \\implies\n",
        "\\|x_1 - x^* \\| \\leq c_1 c_2 \\|x_0 - x^*\\|^2\n",
        "\\leq c_1 c_2 \\left(\\frac{\\alpha}{c_1 c_2}\\right)\\|x_0 - x^*\\| = \\alpha \\|x_0 - x^*\\| \\leq \\|x_0 - x^*\\| \\hspace{5pt}(*)$\n",
        "\n",
        "$(*)\\implies \\|x_1 - x^* \\| \\leq \\frac{\\alpha}{c_1 c_2}$\n",
        "\n",
        "$(*)\\implies \\|x_1 - x^* \\| \\leq \\epsilon$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAcGVQFLGAt_"
      },
      "source": [
        "$\\bold{7)}$\n",
        "\n",
        "From 5\\),\\\n",
        "$x_k \\in B(x^*, \\epsilon) \\implies \\|x_{k+1} - x^* \\| \\leq c_1 c_2 \\|x_k - x^* \\| ^2$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl5dr7feGAt_"
      },
      "source": [
        "$\\bold{8)}$\n",
        "\n",
        "$x_k \\in B(x^*, \\epsilon) \\implies \\|x_{k+1} - x^* \\| \\leq c_1 c_2 \\|x_k - x^* \\| ^2$\n",
        "\n",
        "Using 6\\)\\\n",
        "$x_k \\in B(x^*, \\epsilon) \\land \\|x_k - x^* \\| \\leq \\frac{\\alpha}{c_1 c_2} \\implies x_{k+1} \\in B(x^*, \\epsilon) \\land \\|x_{k+1} - x^* \\| \\leq \\frac{\\alpha}{c_1 c_2}$\n",
        "\n",
        "6\\) is recursive, so $x_0 \\in B(x^*, \\epsilon) \\land \\|x_0 - x^* \\| \\leq \\frac{\\alpha}{c_1 c_2} \\implies \\forall k \\in \\mathbb{N}, \\, x_k \\in B(x^*, \\epsilon) \\land \\|x_k - x^* \\| \\leq \\frac{\\alpha}{c_1 c_2}$\n",
        "\n",
        "Using 7\\)\\\n",
        "$\\forall k \\in \\mathbb{N}, \\, x_k \\in B(x^*, \\epsilon) \\implies\n",
        "\\forall k \\in \\mathbb{N}, \\, \\|x_{k+1} - x^* \\| \\leq c_1 c_2 \\|x_k - x^* \\| ^2$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BexwMwLGAt_"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Part 3: A deeper dive into neural network implementations [3 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PbtiHBzQGAt_"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qqAmocXUGAuA"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "parent_dir = os.path.dirname(current_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n_biS0LGAuA",
        "outputId": "b3a50388-8eb0-440c-9739-3716783896e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Download datasets\n",
        "train_set_mnist = torchvision.datasets.MNIST(root=parent_dir, download=True,\n",
        "                                         train=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "test_set_mnist = torchvision.datasets.MNIST(root=parent_dir,download=True,\n",
        "                                        train=False,transform=transforms.Compose([transforms.ToTensor()]),)\n",
        "\n",
        "train_set_cifar = torchvision.datasets.CIFAR10(root=parent_dir, download=True,\n",
        "                                         train=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "test_set_cifar = torchvision.datasets.CIFAR10(root=parent_dir,download=True,\n",
        "                                        train=False,transform=transforms.Compose([transforms.ToTensor()]),)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L92Vh3bGAuA",
        "outputId": "2279a616-2922-4c5e-abfa-c843f8d86a20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e07e2b85850>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Set seed\n",
        "SEED = '01877978'\n",
        "np.random.seed(int(SEED))\n",
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DiTu7HrGAuH"
      },
      "source": [
        "***\n",
        "\n",
        "### Part 3.1: Implementations [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXAP1uoRGAuH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gupXrnJoGAuH"
      },
      "outputs": [],
      "source": [
        "# You can of course add more cells of both code and markdown. Please remember to comment the code and explain your reasoning. Include docstrings. Tutorial provide a good example of how to style your code.\n",
        "# Although not compulsory you could challenge yourself by using object oriented programming to structure your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54G4Mlw9GAuH"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, dim, nclass, width, depth):\n",
        "        super().__init__()\n",
        "        self.input_layers = nn.ModuleList()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "\n",
        "        self.act_fn = nn.ReLU()\n",
        "        self.output_layer = nn.Linear(width, nclass)\n",
        "\n",
        "        self.input_layers.append(nn.Flatten())\n",
        "        self.input_layers.append(nn.Linear(dim, width))\n",
        "\n",
        "        for n in range(depth):\n",
        "            self.hidden_layers.append(nn.Linear(width, width))\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input.float()\n",
        "        for layer in self.input_layers:\n",
        "            x = layer(x)\n",
        "            x = self.act_fn(x)\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "            x = self.act_fn(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4l56HwHGAuH"
      },
      "outputs": [],
      "source": [
        "def loading_data(batch_size, train_set, test_set):\n",
        "        train_dataloader = DataLoader(dataset = train_set, batch_size = batch_size, shuffle = True)\n",
        "        test_dataloader = DataLoader(dataset = test_set, batch_size = batch_size, shuffle = True)\n",
        "        return train_dataloader, test_dataloader\n",
        "\n",
        "def train_epoch(trainloader, net, optimizer, criterion):\n",
        "    running_loss = 0\n",
        "    for inputs, labels in trainloader:\n",
        "        optimizer.zero_grad()  # Zero gradients\n",
        "        outputs = net(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Calculate loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return running_loss / len(trainloader)\n",
        "\n",
        "def train_test(testloader, net, criterion):\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking for evaluation\n",
        "        for inputs, labels in testloader:\n",
        "            outputs = net(inputs)  # Forward pass\n",
        "            total_loss += criterion(outputs, labels).item()\n",
        "            predicted = torch.argmax(outputs, dim = 1)\n",
        "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
        "\n",
        "    average_loss = total_loss / len(testloader)\n",
        "    accuracy = correct / len(testloader.dataset)\n",
        "\n",
        "    return average_loss, 1 - accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2liw1HXGAuH",
        "outputId": "2b31a599-340a-427a-d787-411972cd08bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 000 | Train Loss: 1.928 |Test Loss: 1.803 | Test Error: 0.6443\n",
            "Epoch: 001 | Train Loss: 1.737 |Test Loss: 1.672 | Test Error: 0.5995\n",
            "Epoch: 002 | Train Loss: 1.659 |Test Loss: 1.648 | Test Error: 0.5867\n",
            "Epoch: 003 | Train Loss: 1.608 |Test Loss: 1.6 | Test Error: 0.5744\n",
            "Epoch: 004 | Train Loss: 1.569 |Test Loss: 1.55 | Test Error: 0.5481\n",
            "Epoch: 005 | Train Loss: 1.531 |Test Loss: 1.561 | Test Error: 0.5606\n",
            "Epoch: 006 | Train Loss: 1.5 |Test Loss: 1.53 | Test Error: 0.542\n",
            "Epoch: 007 | Train Loss: 1.471 |Test Loss: 1.486 | Test Error: 0.5251\n",
            "Epoch: 008 | Train Loss: 1.449 |Test Loss: 1.471 | Test Error: 0.5208\n",
            "Epoch: 009 | Train Loss: 1.43 |Test Loss: 1.452 | Test Error: 0.5091\n",
            "Epoch: 010 | Train Loss: 1.407 |Test Loss: 1.436 | Test Error: 0.5065\n",
            "Epoch: 011 | Train Loss: 1.394 |Test Loss: 1.439 | Test Error: 0.5083\n",
            "Epoch: 012 | Train Loss: 1.371 |Test Loss: 1.449 | Test Error: 0.5158\n",
            "Epoch: 013 | Train Loss: 1.357 |Test Loss: 1.404 | Test Error: 0.4982\n",
            "Epoch: 014 | Train Loss: 1.345 |Test Loss: 1.424 | Test Error: 0.5045\n",
            "Epoch: 015 | Train Loss: 1.332 |Test Loss: 1.404 | Test Error: 0.4927\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[263], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 10\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     test_loss, test_err \u001b[38;5;241m=\u001b[39m train_test(testloader, net, criterion)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.04\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m |Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.04\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Test Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_err\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.04\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[221], line 13\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(trainloader, net, optimizer, criterion)\u001b[0m\n\u001b[0;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)  \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(trainloader)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adamax.py:119\u001b[0m, in \u001b[0;36mAdamax.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    115\u001b[0m     differentiable \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    117\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, grads, exp_avgs, exp_infs, state_steps)\n\u001b[1;32m--> 119\u001b[0m     \u001b[43madamax\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_infs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforeach\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adamax.py:223\u001b[0m, in \u001b[0;36madamax\u001b[1;34m(params, grads, exp_avgs, exp_infs, state_steps, foreach, maximize, differentiable, has_complex, eps, beta1, beta2, lr, weight_decay)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamax\n\u001b[1;32m--> 223\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_infs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adamax.py:283\u001b[0m, in \u001b[0;36m_single_tensor_adamax\u001b[1;34m(params, grads, exp_avgs, exp_infs, state_steps, eps, beta1, beta2, lr, weight_decay, maximize, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    278\u001b[0m norm_buf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[0;32m    279\u001b[0m     [exp_inf\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39madd_(eps)\u001b[38;5;241m.\u001b[39munsqueeze_(\u001b[38;5;241m0\u001b[39m)], \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    280\u001b[0m )\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m differentiable:\n\u001b[1;32m--> 283\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    285\u001b[0m     exp_inf\u001b[38;5;241m.\u001b[39mcopy_(torch\u001b[38;5;241m.\u001b[39mamax(norm_buf, \u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainloader, testloader = loading_data(64, train_set_cifar, test_set_cifar)\n",
        "\n",
        "num_epochs = 100\n",
        "net = Net(3072, 10, 100, 2)\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adamax(net.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(trainloader, net, optimizer, criterion)\n",
        "    test_loss, test_err = train_test(testloader, net, criterion)\n",
        "    print(f'Epoch: {epoch:03} | Train Loss: {train_loss:.04} |Test Loss: {test_loss:.04} | Test Error: {test_err:.04}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhbpCe3GGAuH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcPD3YpGGAuH"
      },
      "source": [
        "***\n",
        "\n",
        "### Part 3.2: Numerical exploration [2 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqihFiBbGAuH"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwEv-dSNGAuI"
      },
      "source": [
        "| Depth          | Train Loss     | Test Loss       |\n",
        "| -------------- | -------------- | -------------- |\n",
        "| 1              | Row 1, Col 2   | Row 1, Col 3   |\n",
        "| 5              | Row 2, Col 2   | Row 2, Col 3   |\n",
        "| 10             |                | Row 3, Col 3   |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSvlCWm5GAuI"
      },
      "source": [
        "| Width | Train loss | Test loss |\n",
        "|-------|------------|-----------|\n",
        "| 4     |            |           |\n",
        "| 8     |            |           |\n",
        "| 16    |            |           |\n",
        "| 32    |            |           |\n",
        "| 64    |            |           |\n",
        "| 128   |            |           |\n",
        "| 256   |            |           |\n",
        "| 512   |            |           |\n",
        "| 1024  |            |           |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0lFXqBmGAuI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnK7kyJrGAuI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY_HKpxHGAuI"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Part 4: The link between Neural Networks and Gaussian Processes [8 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2ORgXKaGAuI"
      },
      "source": [
        "### Part 4.1: Proving the relationship between a Gaussian process and a neural network [4 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s-jnqQdGAuI"
      },
      "source": [
        "### Task 1: Proper weight scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3IDqP2VGAuI"
      },
      "source": [
        "\\begin{align*}\n",
        "Var(f_i^{(l)}) &=  Var\\left( \\sum_{j=1}^{N_{l-1}} w_{ij}^{(l)}g_j^{(l)}(x) + b_i^{(l)} \\right)\\\\\n",
        "&= Cov\\left(\\sum_{j=1}^{N_{l-1}} w_{ij}^{(l)}g_j^{(l)}(x) + b_i^{(l)}, \\sum_{k=1}^{N_{l-1}} w_{ik}^{(l)}g_k^{(l)}(x) + b_i^{(l)} \\right)\\\\\n",
        "&= \\sigma_b + \\sum_{j=1}^{N_{l-1}} \\sum_{k=1}^{N_{l-1}} Cov \\left( w_{ij}^{(l)}g_j^{(l)}(x),  w_{ik}^{(l)}g_k^{(l)}(x)\\right)\\\\\n",
        "&= \\sigma_b + C_w^{(l)} \\sum_{j=1}^{N_{l-1}} Var(g_j^{(l)}(x))\\\\\n",
        "&= \\sigma_b + N_{l-1} C_w^{(l)} Var(g_j^{(l)}(x))\\\\\n",
        "&= \\sigma_b + N_{l-1} C_w^{(l)} Var(g_j^{(l)}(x)) \\hspace{10pt} (*)\n",
        "\\end{align*}\n",
        "$C_w^{(l)} = \\frac{\\sigma_w}{N_{l-1}} \\implies Var(f_i^{(l)}) = \\frac{1}{\\sigma_b} + \\sigma_w Var(g_j^{(l)}(x))$, constant w.r.t  $N_{l-1}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycjOg0QXGAuI"
      },
      "source": [
        "### Task 2: Derive the GP relation for a single hidden layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2wA8JTpGAuI"
      },
      "source": [
        "$f^{(2)} = W^{(2)}g^{(1)}(x) + b^{(2)}$\n",
        "\n",
        "$f_i^{(2)} =  \\sum_{j=1}^{N_1} w_{ij}^{(2)}g_j^{(1)}(x) + b_i^{(2)}$\\\n",
        "$g_i$ independent $\\forall i$ and $w_{ij}$ independent $\\forall i, j \\, : \\, i \\neq j \\implies$ $w_{ij}g_i$ independent $\\forall i$\\\n",
        "similarly $\\forall i$, $w_{ij}$ is identically distributed\n",
        "\n",
        "By CLT, for large $N_1, \\hspace{28pt} \\frac{1}{\\sqrt{N_1}} \\sum_{j=1}^{N_1} w_{ij}^{(2)}g_j^{(1)}(x) \\hspace{8pt} \\sim N\\left(\\mu, C_w Var[g_j^{(1)}(x)]\\right)$\n",
        "\n",
        "$\\hspace{107pt} \\sum_{j=1}^{N_1} w_{ij}^{(2)}g_j^{(1)}(x) \\hspace{25pt} \\sim N\\left(0, \\sigma_w Var[g_j^{(1)}(x)]\\right)$\n",
        "\n",
        "$\\hspace{78pt} f_i^{(2)} = \\sum_{j=1}^{N_1} w_{ij}^{(2)}g_j^{(1)}(x) + b_i^{(2)}  \\sim N\\left(\\mu, \\sigma_w Var[g_j^{(1)}(x)]  + \\sigma_b \\right)$\n",
        "\n",
        "Let $\\bold{x}$ be the vector of different inputs\n",
        "\n",
        "$f_i^{(2)}(\\bold{x}) = \\begin{pmatrix}\n",
        "    e_i^T \\\\\n",
        "    e_i^T \\\\\n",
        "    ...   \\\\\n",
        "    e_i^T\n",
        "\\end{pmatrix}\n",
        "W\n",
        "g_i^{(1)}(\\bold{x})\n",
        "= \\sum\\limits_{j=0}^{N_1} w_{ij} \\begin{pmatrix}\n",
        "    g_i(x^{(1)}) \\\\\n",
        "    g_i(x^{(2)}) \\\\\n",
        "    ...   \\\\\n",
        "    g_i(x^{(M)})\n",
        "\\end{pmatrix}\n",
        "= \\sum\\limits_{j=0}^{N_1} G_j(\\bold{x})$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEsztD74GAuI"
      },
      "source": [
        "\\begin{align*}\n",
        "Cov[f_i^{(2)}(x), f_i^{(2)}(x')] &=  Cov\\left[\\sum_{j=1}^{N_1} w_{ij}^{(2)}g_j^{(1)}(x) + b_i^{(2)},  \\sum_{k=1}^{N_1} w_{ik}^{(2)}g_k^{(1)}(x') + b_i^{(2)}\\right]\\\\\n",
        "&=\\sigma_b + \\sum_{j=1}^{N_1} \\sum_{k=1}^{N_1} Cov\\left[ w_{ij}^{(2)}g_j^{(1)}(x), w_{ik}^{(2)}g_k^{(1)}(x') \\right]\\\\\n",
        "&=\\sigma_b + C_w \\sum_{j=1}^{N_1} Cov\\left[g_j^{(1)}(x), g_j^{(1)}(x')\\right]\\\\\n",
        "&=\\sigma_b + C_w \\sum_{j=1}^{N_1} Cov\\left[\\phi(f^{(1)})(x), \\phi(f^{(1)})(x')\\right]\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAfStspiGAuI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9rPuCDQGAuI"
      },
      "source": [
        "### Task 3: Why in succession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpAOg92GGAuI"
      },
      "source": [
        "We take limits in succession so now the inputs to the second order are $g(f(x))$, with $f(x)$ distributed normally, (since it tends toward normal in distribution), so we can say that the $f_i$ are independent (uncorrelated $\\implies$ independent for normally distribtuted) and therefore the gs are independent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b-ksAQtGAuI"
      },
      "source": [
        "### Task 4: Derive the GP relation for multiple hidden layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SS6-tkuGAuI"
      },
      "source": [
        "Show $f_j^{(l-1)}$ is a GP $\\implies f_j^{(l)}$ is a GP.\n",
        "\n",
        "$f_j^{(l)}(\\bold{x}) =\n",
        "\\sum\\limits_{j=0}^{N_{l-1}} w_{ij} \\begin{pmatrix}\n",
        "    g_i^{(l-1)}(x^{(1)}) \\\\\n",
        "    g_i^{(l-1)}(x^{(2)}) \\\\\n",
        "    ...   \\\\\n",
        "    g_i^{(l-1)}(x^{(M)})\n",
        "\\end{pmatrix}\n",
        "$\n",
        "\n",
        "By CLT, $f_j^{(l)}(\\bold{x})$ is a GP.\n",
        "\n",
        "$E\\left[ f_j^{(l)}(\\bold{x})\\right] = E\\left[\n",
        "\\sum\\limits_{j=0}^{N_{l-1}} w_{ij} \\begin{pmatrix}\n",
        "    g_j^{(l-1)}(x^{(1)}) \\\\\n",
        "    g_j^{(l-1)}(x^{(2)}) \\\\\n",
        "    ...   \\\\\n",
        "    g_j^{(l-1)}(x^{(M)})\n",
        "\\end{pmatrix}\n",
        "\\right] =\n",
        "\\sum\\limits_{j=0}^{N_{l-1}} E[w_{ij}] E\\left[\n",
        "\\begin{pmatrix}\n",
        "    g_j^{(l-1)}(x^{(1)}) \\\\\n",
        "    g_j^{(l-1)}(x^{(2)}) \\\\\n",
        "    ...   \\\\\n",
        "    g_j^{(l-1)}(x^{(M)})\n",
        "\\end{pmatrix}\n",
        "\\right] = \\bold{0}$\n",
        "\n",
        "$Cov\\left[ f_j^{(l)}(x), f_j^{(l)}(x') \\right] =\n",
        "E\\left[ f_j^{(l)}(x)f_j^{(l)}(x')\\right] =\\\\\n",
        "E\\left[ \\sum\\limits_{j=0}^{N_{l-1}} \\sum\\limits_{k=0}^{N_{l-1}} \\left(w_{ij} g_j^{(l-1)}(x) + b_i(x) \\right) \\left(g_k^{(l-1)}(x') + b_i(x')\\right) \\right] = \\\\\n",
        "\\sum\\limits_{j=0}^{N_{l-1}} \\sum\\limits_{k=0}^{N_{l-1}} E[ w_{ij} w_{ik}] E[g_j^{(l-1)}(x) g_k^{(l-1)}(x') ]  =\\\\\n",
        "C_w \\sum\\limits_{k=0}^{N_{l-1}} E[g_j^{(l-1)}(x) g_k^{(l-1)}(x') ]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjVF_zA4GAuJ"
      },
      "source": [
        "***\n",
        "\n",
        "### Part 4.2: Analysing the performance of the Gaussian process and a neural network [4 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x4fQUFHGAuJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "hop02aZ5GAuJ"
      },
      "outputs": [],
      "source": [
        "# Please use float64 as default dtype for this part of the assignment\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "# Another hint: when  computing [ K^L(X,X) + noise^2 Id ]^-1 y and  [ K^L(X,X) + noise^2 Id ]^-1 K^L(X,X*)\n",
        "# You can TRY cholesky solve as it should be p.d. (except case for numerical errors) - maybe you can use try:/except:\n",
        "# You can also try to enforce symmetry in posterior covariance by doing (K + K.t())/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqbEiXORGAuJ"
      },
      "outputs": [],
      "source": [
        "# You can of course add more cells of both code and markdown."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_cifar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVD75h-QJMxH",
        "outputId": "7ce24457-9450-462e-a4db-6d5babed0bc5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 50000\n",
              "    Root location: /\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "lCd1rpVMGAuJ"
      },
      "outputs": [],
      "source": [
        "target_mapping_dic = {0:-.5,\n",
        "                      1:.5}\n",
        "target_mapping = lambda x : target_mapping_dic.get(x,x)\n",
        "target_mapping = np.vectorize (target_mapping)\n",
        "train_set_cifar.targets = torch.tensor(train_set_cifar.targets, dtype = torch.float64)\n",
        "train_set_cifar.targets = target_mapping(train_set_cifar.targets)\n",
        "\n",
        "boolean_mask = (train_set_cifar.targets == 0.5) | (train_set_cifar.targets ==-0.5)\n",
        "train_set_cifar.data = train_set_cifar.data[boolean_mask]\n",
        "train_set_cifar.targets = [boolean_mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nasn96REGAuJ"
      },
      "outputs": [],
      "source": [
        "sigma_b = 1\n",
        "sigma_w = 1\n",
        "\n",
        "def theta(x1, x2 ,l):\n",
        "    K_x_x = kernel(x1, x1, l, sigma_b, sigma_w)\n",
        "    K_xs_x = kernel(x1, x2, l, sigma_b, sigma_w)\n",
        "    K_xs_xs = kernel(x2, x2, l, sigma_b, sigma_w)\n",
        "\n",
        "    return torch.arccos(K_xs_x / torch.sqrt(K_x_x * K_xs_xs))\n",
        "\n",
        "def kernel(x1, x2, l, sigma_b, sigma_w):\n",
        "    \"\"\"\n",
        "    This function should take as input the number of layers L, Ïƒ2\n",
        "    w, Ïƒ2, b and two datasets X1, X2 of size M1 Ã— N0 and M2 Ã— N0 where\n",
        "    N0 is, as before, the dimension of the input and M1 and M2 are\n",
        "    the number of datapoints in the first and second dataset, respectively.\n",
        "    The output from this function should be a matrix KL where each element KL_ij\n",
        "    is the kernel function applied to element i from X1 and element j from X2\n",
        ".\n",
        "    \"\"\"\n",
        "    N0 = x1.shape[1]\n",
        "    if l == 0:\n",
        "        return sigma_b + sigma_w *(torch.matmul(x1, x2) / N0)\n",
        "\n",
        "    K_x_x = kernel(x1, x1, l-1, sigma_b, sigma_w)\n",
        "    K_xs_xs = kernel(x2, x2, l-1, sigma_b, sigma_w)\n",
        "\n",
        "    return (sigma_b + sigma_w / (2 * torch.pi) * torch.sqrt(K_x_x * K_xs_xs) *\n",
        "    torch.sin(theta(x1, x2 ,l-1)) + (torch.pi - theta(x1, x2 ,l-1)) * torch.cos(theta(x1, x2 ,l-1)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def kernel_vectorized(x1, x2, l, sigma_b, sigma_w):\n",
        "    N0 = x1.shape[1]\n",
        "    if l == 0:\n",
        "        return sigma_b + sigma_w *(torch.inner(x1, x2) / N0)\n",
        "\n",
        "    K_x_x = kernel_vectorized(x1, x1, l-1, sigma_b, sigma_w)\n",
        "    K_xs_xs = kernel_vectorized(x2, x2, l-1, sigma_b, sigma_w)\n",
        "\n",
        "    x_x_diag = torch.diag(K_x_x)\n",
        "    xs_xs_diag = torch.diag(K_xs_xs)\n",
        "\n",
        "    return (sigma_b + sigma_w / (2 * torch.pi) * torch.sqrt(torch.outer(x_x_diag, xs_xs_diag)) *\n",
        "    torch.sin(theta(x1, x2 ,l-1)) + (torch.pi - theta(x1, x2 ,l-1)) * torch.cos(theta(x1, x2 ,l-1)))"
      ],
      "metadata": {
        "id": "aSC5lb-XGdXr"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = torch.tensor(train_set_cifar.data)[:1000]\n",
        "training_targets = train_set_cifar.targets[:1000]"
      ],
      "metadata": {
        "id": "AAHCgHwiIhIn"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Jb9C8bzuGAuJ"
      },
      "outputs": [],
      "source": [
        "#TESTING\n",
        "X1 = training_data[:100]\n",
        "X2 = training_data[100:130]\n",
        "\n",
        "X1 = X1.flatten(start_dim = 1)\n",
        "X2 = X2.flatten(start_dim = 1)\n",
        "sigma_b = 1\n",
        "sigma_w = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "njyxEHpHGAuJ",
        "outputId": "0d0ea7b8-7c23-4084-930e-be54bafac4d2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RecursionError",
          "evalue": "maximum recursion depth exceeded in comparison",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-c32ddac1debb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkernel_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-76-885ed240a9b3>\u001b[0m in \u001b[0;36mkernel_vectorized\u001b[0;34m(x1, x2, l, sigma_b, sigma_w)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msigma_b\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigma_w\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mK_x_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mK_xs_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-885ed240a9b3>\u001b[0m in \u001b[0;36mkernel_vectorized\u001b[0;34m(x1, x2, l, sigma_b, sigma_w)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msigma_b\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigma_w\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mK_x_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mK_xs_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-885ed240a9b3>\u001b[0m in \u001b[0;36mkernel_vectorized\u001b[0;34m(x1, x2, l, sigma_b, sigma_w)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     return (sigma_b + sigma_w / (2 * torch.pi) * torch.sqrt(torch.outer(x_x_diag, xs_xs_diag)) * \n\u001b[0;32m---> 13\u001b[0;31m     torch.sin(theta(x1, x2 ,l-1)) + (torch.pi - theta(x1, x2 ,l-1)) * torch.cos(theta(x1, x2 ,l-1)))\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-3f36e96f418c>\u001b[0m in \u001b[0;36mtheta\u001b[0;34m(x1, x2, l)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mK_x_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mK_xs_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mK_xs_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-3f36e96f418c>\u001b[0m in \u001b[0;36mkernel\u001b[0;34m(x1, x2, l, sigma_b, sigma_w)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msigma_b\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigma_w\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mK_x_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mK_xs_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "... last 1 frames repeated, from the frame below ...\n",
            "\u001b[0;32m<ipython-input-10-3f36e96f418c>\u001b[0m in \u001b[0;36mkernel\u001b[0;34m(x1, x2, l, sigma_b, sigma_w)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msigma_b\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigma_w\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mK_x_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mK_xs_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded in comparison"
          ]
        }
      ],
      "source": [
        "kernel_vectorized(X1, X2, 3, sigma_b, sigma_w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88tufQ9HGAuJ",
        "outputId": "8880d391-c29d-48cf-d427-961f35073995"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0238, 1.0195, 1.0589,  ..., 1.0788, 1.0716, 1.0762],\n",
              "        [1.0189, 1.0605, 1.0322,  ..., 1.0570, 1.0459, 1.0531],\n",
              "        [1.0410, 1.0010, 1.0309,  ..., 1.0228, 1.0150, 1.0186],\n",
              "        ...,\n",
              "        [1.0133, 1.0254, 1.0091,  ..., 1.0684, 1.0000, 1.0072],\n",
              "        [1.0599, 1.0182, 1.0143,  ..., 1.0729, 1.0638, 1.0420],\n",
              "        [1.0208, 1.0016, 1.0127,  ..., 1.0583, 1.0433, 1.0303]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "sigma_b + sigma_w *(torch.inner(X1, X2) / X1.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqZzMeeNGAuK",
        "outputId": "0e45eed2-d97d-43a5-d2f3-2953146abb34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[0.2314, 0.1686, 0.1961,  ..., 0.6196, 0.5961, 0.5804],\n",
              "          [0.0627, 0.0000, 0.0706,  ..., 0.4824, 0.4667, 0.4784],\n",
              "          [0.0980, 0.0627, 0.1922,  ..., 0.4627, 0.4706, 0.4275],\n",
              "          ...,\n",
              "          [0.8157, 0.7882, 0.7765,  ..., 0.6275, 0.2196, 0.2078],\n",
              "          [0.7059, 0.6784, 0.7294,  ..., 0.7216, 0.3804, 0.3255],\n",
              "          [0.6941, 0.6588, 0.7020,  ..., 0.8471, 0.5922, 0.4824]],\n",
              " \n",
              "         [[0.2431, 0.1804, 0.1882,  ..., 0.5176, 0.4902, 0.4863],\n",
              "          [0.0784, 0.0000, 0.0314,  ..., 0.3451, 0.3255, 0.3412],\n",
              "          [0.0941, 0.0275, 0.1059,  ..., 0.3294, 0.3294, 0.2863],\n",
              "          ...,\n",
              "          [0.6667, 0.6000, 0.6314,  ..., 0.5216, 0.1216, 0.1333],\n",
              "          [0.5451, 0.4824, 0.5647,  ..., 0.5804, 0.2431, 0.2078],\n",
              "          [0.5647, 0.5059, 0.5569,  ..., 0.7216, 0.4627, 0.3608]],\n",
              " \n",
              "         [[0.2471, 0.1765, 0.1686,  ..., 0.4235, 0.4000, 0.4039],\n",
              "          [0.0784, 0.0000, 0.0000,  ..., 0.2157, 0.1961, 0.2235],\n",
              "          [0.0824, 0.0000, 0.0314,  ..., 0.1961, 0.1961, 0.1647],\n",
              "          ...,\n",
              "          [0.3765, 0.1333, 0.1020,  ..., 0.2745, 0.0275, 0.0784],\n",
              "          [0.3765, 0.1647, 0.1176,  ..., 0.3686, 0.1333, 0.1333],\n",
              "          [0.4549, 0.3686, 0.3412,  ..., 0.5490, 0.3294, 0.2824]]]),\n",
              " 6.0)"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set_cifar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Kqe-Yi6GAuK",
        "outputId": "ffecc7cf-25e9-42c9-c24b-f6aa70a22f72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function torch._VariableFunctionsClass.arccos>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.arccos"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
=======
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1 - Mathematics for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CID: 01877978\n",
    "\n",
    "**Colab link:** insert colab link here\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Quickfire questions [3 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 (True risk / Empirical risk):\n",
    "\n",
    "True risk $R(f) = \\mathbb{E}_D[L(f(\\bold{x},\\bold{y}))]$\\\n",
    "Empirical risk $\\hat{R}(f) = \\frac{1}{N}\\sum_{i=1}^N L(f(\\bold{x}^i),\\bold{y}^i)$\n",
    "\n",
    "The True risk $R(f)$ can only be evaluated by knowing the joint distrubution of ($\\bold{x}, \\bold{y})$, so is instead approximated by using realisations of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 ('Large' or 'rich' hypothesis class):\n",
    "Hypothesis class $F$ is the set of functions which can be used to model $(\\bold{x}, f(\\bold{x}))$\\\n",
    "Downfalls are it is harder to optimize over as you have introduced more parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 (Dataset splitting):\n",
    "\n",
    "No, you chose the model with the maximum validation set score.\n",
    "\n",
    "If all the model validation set score was i.i.d $\\sim N(\\mu, \\sigma^2)$,the maximum would be distributed with $ P(X_{max} \\leq x) = \\prod_{i=1}^{N}[P(X_i \\leq x)] = \\Phi^n(x)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 (Occamâ€™s razor):\n",
    "\n",
    "Among competing hypotheses that explain known observations equally well, we should choose the simplest one. In relation to data such as images, this could include "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5 (Generalisation error):\n",
    "\n",
    "You would expect a small generalisation error on a good model, i.e. it is not overfit on the training set so it performly similarly in training as in testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6 (Rademacher complexity pt1):\n",
    "\n",
    "High empirical rademacher complexity means for a given sample $S = \\{ z_1, z_2, ... z_m\\}$, that our function class does not contain a function that maps the input well to random noise. This means our function class is not too wide that it could fit to data which is unpredictable (random noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7 (Rademacher complexity pt2):\n",
    "\n",
    "Downsides of the bound in (73) is it only applies in finite circumstances and it only crudely considers the size of the function class and penalizes for this (log of cardinality). Whereas in the rademacher complexity generlisation bound a function class which could better approximate random noise would have a greater empirical rademacher complexity and therefore have a higher bound on the true rademacher complexity (i.e we are better punishing the function class for its ability to overfit rather than just its cardinality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8 (Regularisation term in the loss function):\n",
    "**COME BACK**\n",
    "Regularisation term in the loss function can encourage the model to adopt a simpler complexity in the training. This can lead to a model with better generalisation ability and which is not overfit to the training data. Also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9 (Momentum gradient descent):\n",
    "\n",
    "Momentum helps the gradient descent escape local minima pits (the momentum accumulated when going in to the local minima helps carry it out). It doesn't take very small steps at gentle gradients, the descent is generally smoother (the gradient of step taken is correlated to gradient of past step taken). When the gradient is oscillating it will appropiately slow down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10 (Adam):\n",
    "Using estimations for first and second moments of gradient. The recursive step is :\n",
    "\\begin{align}\n",
    "&t &&= t + 1\\\\\n",
    "&m_t &&= \\beta_1 m_{t-1} + (1 - \\beta_1) \\left(\\frac{\\delta L}{\\delta w_t}\\right)\\\\\n",
    "&v_t &&= \\beta_2 v_{t-1} + (1 - \\beta_2) \\left(\\frac{\\delta L}{\\delta w_t}\\right)^2\\\\\n",
    "&\\hat{m}_t &&= m_t / (1 - \\beta_1^t)\\\\\n",
    "&\\hat{v}_t &&= v_t / (1 - \\beta_2^t)\\\\\n",
    "&w_t &&= w_{t-1} - \\alpha \\left(\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + e}\\right)\n",
    "\\end{align}\n",
    "\n",
    "Here $m_t$ is our estimate for the first moment of the gradient, and $v_t$ the estimate of the second moment of the gradient (gradient refers to gradient of loss function with respect to parameters). Adjust to $\\hat{m}(t)$ and $\\hat{v}_t$ to adjust for the bias. \n",
    "\n",
    "There is a momentum used in the estimation for the calculation of $v_t$ and $m_t$ which include all previous estimates (although the weights experience exponential deacay). The gradient evaluated at time t is also used in the calculation for m_t, so you take a step in the direction of the greatest gradient up to time t (adjusting for the deacy of the weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11 (AdaGrad):\n",
    "\n",
    "Gradient descent with fixed learning rate looks like $x_{t+1} = x_t - \\alpha \\nabla f(x_t)$\n",
    "\n",
    "Whereas AdaGrad looks like $x_{t+1} = x_t - \\alpha \\,G_t^{-1/2} \\, \\nabla f(x_t)$\\\n",
    "With $G_t = \\sum_{k=0}^t \\left(\\nabla f(x_k) \\otimes \\nabla f(x_k)\\right)$, similar to Adam this is an estimation of the second moment of the gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12 (Decaying Learning Rate):\n",
    "\n",
    "A decaying learning rate means you will take smaller steps at further iterations, when you would be closer to the minimasing parameters. This means along the loss landscape as you are closer to the minima you are taking smaller steps. Desirable since you would want want smaller more precise steps when closer to the minima. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "***\n",
    "\n",
    "## Part 2: Short-ish proofs [6 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 2.1: Bounds on the risk [1 point]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoeffdings inequality\\\n",
    "**SECOND PART: Need be done**\n",
    "\n",
    "Let $X_1, X_2, X_3,..,X_m$ independent, taking values in $[a_i, b_i]$. With $S_m = \\sum_{i=1}^m X_i$.\\\n",
    "$\\forall \\epsilon > 0$\n",
    "\n",
    "$P(S_n - E[S_n] \\geq \\epsilon) \\leq exp\\left \\{ \\begin{aligned}\n",
    "    -\\frac{2\\epsilon^2}{\\sum_{i=1}^n(b_i - a_i)^2}\n",
    "    \\end{aligned}\n",
    "    \\right\\}$\n",
    "\n",
    "Viewing $L(f(x_i), y_i)$ i.i.d random variables (a function of the join distribution $(\\bold{x}, \\bold{y})$)\\\n",
    "Noting $0 \\leq L(f(x_i), y_i) \\leq 1, \\forall \\omega \\in \\Omega$, we take $b_i = 1, a_i = 0, \\forall i$\\\n",
    "and as they are i.i.d, $\\mathbb{E}[S_n] = \\mathbb{E}[L(f(\\bold{x}), \\bold{x})]$\n",
    "\n",
    " $\\forall \\epsilon$, we have\n",
    "$P(\\frac{1}{N}S_n - \\frac{1}{N} E[S_n] \\geq \\epsilon)  = p(S_n - E[S_n] \\geq N \\epsilon) \\leq exp\\left \\{ \\begin{aligned}\n",
    "    -\\frac{2(N \\epsilon)^2}{\\sum_{i=1}^N1^2}\n",
    "    \\end{aligned}\n",
    "    \\right\\}\n",
    "=exp\\left \\{ \\begin{aligned}\n",
    "    -2 N \\epsilon^2\n",
    "    \\end{aligned}\n",
    "    \\right\\}$\n",
    "\n",
    "\n",
    "3\\) d\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Question 2.2: On semi-definiteness [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(y) \\geq f(x) + \\nabla f(x)^T(y-x)$\n",
    "\n",
    "Let $g(t) = f(x+tv)$ \n",
    "\n",
    "$g'(t) = v^T \\, \\nabla f(x + vt)$\n",
    "\n",
    "$g''(t) = v^T \\cdot\\nabla^2 f(x + tv) \\cdot v$\n",
    "\n",
    "Showing g is convex.\\\n",
    "$\\forall t_1, t_2 \\in \\mathbb{R},\\alpha \\in (0,1),\\\\\n",
    "g(\\alpha t_1  + (1 - \\alpha)t_2) = f(\\bold{x} + (\\alpha t_1  + (1 - \\alpha)t_2)\\bold{v}) = f(\\alpha [\\bold{x} +  t_1 \\bold{v}] + (1-\\alpha)[ \\bold{x} + t_2 \\bold{v}]) \\leq \\alpha f(\\bold{x} +  t_1 \\bold{v}) + (1-\\alpha) f(\\bold{x} + t_2 \\bold{v})\\\\\n",
    "=\\alpha g(t_1) + (1 - \\alpha) g(t_2)$\n",
    "\n",
    "$g$ is convex $\\implies$ $g''(t) \\geq 0$\n",
    "\n",
    "As our choice of $\\bold{v}$ was arbitrary we have $\\forall \\bold{v} \\in \\mathbb{R^d}$, $0 \\leq g''(t) = \\bold{v} ^T \\, \\nabla ^2 f(x + tv) \\, \\bold{v}$\n",
    "\n",
    "So f is positive semi-definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Question 2.3: A quick recap of momentum [1 point]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\bold{1)}$\n",
    "\n",
    "Let $\\bold{v} = \\sum_{k=1}^d \\alpha_k \\bold{q}_k \\Longleftrightarrow Q^T\\bold{v} =\n",
    "Q^T \\sum_{k=1}^d \\alpha_k \\bold{q}_k =\n",
    "\\sum_{k=1}^d \\alpha_k Q^T \\bold{q}_k =\n",
    "\\sum_{k=1}^d \\alpha_k \\bold{e}_k$\n",
    "\n",
    "So $w_k = Q^T(x_k - x^*)$ changes the basis to be in terms of eigenvectors and recentres it so the stationary point is now the zero vector.\\\n",
    "Allows you to obtain rate of convergence in terms of eigenvalues (applying Q has a scalar effect in each basis direction). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)**\\\n",
    "The optimal learning rate achieves speeds up the steps in the slowest eigenvector direction (thereby raising the lower bound of the convergence rate). Downsides are it does not optimize for any \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)**\\\n",
    "Assuming it is diagonalizable, the convergence goes at the rate of the greatest magnitude of eigenvalue.  O\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)**\\\n",
    "$(\\beta + 1 - \\alpha \\lambda_i)^2 - 4 \\beta < 0 \\Longleftrightarrow $ R has complex eigenvalues, R having complex eigenvalues implies they have fixed magnitude $\\sqrt{\\beta}$ which is the rate of convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Question 2.4: Convergence proof [3 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bold{1)}$\n",
    "\n",
    "Iterative step: $x_{k+1} = x_k  - (\\nabla ^2(f(x_k))^{-1} \\nabla f(x_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bold{2)}$\n",
    "\n",
    "With $f(x) = \\frac{1}{2} x^t Q x + b^T x + c$\n",
    "\n",
    "$\\nabla f(x) = Qx + b$\\\n",
    "$\\nabla^2 f(x) = Q$\n",
    " \n",
    "so $x_{k+1} = X_k - Q^{-1} (Qx_k + b) = x_k - x_k + Q^{-1}b = - Q^{-1}b$\n",
    "\n",
    "$- Q^{-1}b = x$, satisfies $x = x - Q^{-1}(Qx + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)**\\\n",
    "The statement means:\n",
    "$f is a somewhat smooth function \\\n",
    "(3 times continuously differentiable)\n",
    "stat. point $x^*$ : $\\nabla^2 f(x^*)$ invertible $ \\hspace{50pt}\\implies$ \n",
    "There is a radius around x such that Newton's method will always  $\\hspace{320pt}$ work and converge quadratically to $x^*$.\n",
    "\n",
    "\\begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "&\\text{The statement means: f is a somewhat smooth function}  \\hspace{100pt} &&&\\text{2nd}\\\\\n",
    "&\\text{(3 times continuously differentiable) stat. point} &&\\implies &&&\\text{2nd} \\\\\n",
    "&x^* : \\nabla^2 f(x^*) \\text{invertible}\n",
    "\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bold{4)}$\n",
    "\n",
    "$\\| x_1 - x^* \\|$, subbing in the definition of $x_1$ \n",
    "\n",
    "$= \\| x_0 - (\\nabla ^2f(x_0))^{-1}(\\nabla f(x_0)) - x^* \\|$\n",
    "\n",
    "$ =\\| (\\nabla ^2f(x_0))^{-1} (\\nabla ^2f(x_0)) \\left[x_0 - (\\nabla ^2f(x_0))^{-1}(\\nabla f(x_0)) - x^* \\right]\\|$\n",
    "\n",
    "$ \\leq \\| (\\nabla ^2f(x_0))^{-1} \\| \\| (\\nabla ^2f(x_0)) (x_0 - x^*) - (\\nabla f(x_0)) \\|\\hspace{10pt}$ Using Lemma 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bold{5)}$\\\n",
    "Choose $\\epsilon > 0 \\, : \\, (\\nabla^2 f(x_0))^{-1}$ exists\n",
    "\n",
    "Taylor series on $\\nabla f(x^*)$ around $x_0$\\\n",
    "$\\exists c \\in (x^*, x_0) \\, : \\, $\\\n",
    "$0 = \\nabla f(x^*) = \\nabla f(x_0) + \\nabla^2 f(x')(x_0 - x^*)$\n",
    "\n",
    "$\\hspace{10.5pt} \\nabla ^2f(x_0) (x_0 - x^*) - \\nabla f(x_0)\\\\\n",
    "= \\nabla ^2f(x_0) (x_0 - x^*) - \\nabla f(x_0) + \\nabla f(x^*)\\\\\n",
    "= \\nabla ^2f(x_0) (x_0 - x^*) - \\nabla f(x_0) + \\nabla f(x_0) + \\nabla^2 f(x')(x^* - x_0)\\\\\n",
    "= \\left(\\nabla ^2f(x_0) - \\nabla^2 f(x')\\right)(x_0 - x^*)$\n",
    "\n",
    "$\\implies \\| \\nabla ^2f(x_0) (x_0 - x^*) - \\nabla f(x_0) \\| \\leq \\|\\left(\\nabla ^2f(x_0) - \\nabla^2 f(x')\\right)(x_0 - x^*) \\| \\leq\n",
    "\\|\\left(\\nabla ^2f(x_0) - \\nabla^2 f(x')\\right)(x_0 - x^*) \\|$\n",
    "\n",
    "$\\nabla f \\text{ twice differentiable} \\land \\| \\nabla^3 f(x) \\| \\leq L \\text{ (for some L)} \\implies \\nabla^2 f \\text{ is lipschitz}$\n",
    "\n",
    "$\\| x_1 - x^* \\| \n",
    "\\leq \\| (\\nabla ^2f(x_0))^{-1} \\| \\| \\left( \\nabla ^2f(x_0) - \\nabla^2 f(x')\\right) (x_0 - x^*) \\|\n",
    "\\leq \\| (\\nabla ^2f(x_0))^{-1} \\| \\| \\nabla^2 f(x_0) - \\nabla^2 f(x') \\| \\| (x_0 - x^*) \\| ^2$\n",
    "\n",
    "$\\leq \\| (\\nabla ^2f(x_0))^{-1} \\| L \\| (x_0 - x^*) \\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bold{ 6)}$\n",
    "\n",
    "Given $x_0 \\in B(x^*, \\epsilon)\\,  : \\, \\|x_0 - x^*\\| \\leq \\frac{\\alpha}{c_1 c_2}$\n",
    "\n",
    "Show $x_1 \\in B(x^*, \\epsilon) \\land \\|x_1 - x^* \\| \\leq \\frac{\\alpha}{c_1 c_2}$\n",
    "\n",
    "\n",
    "\n",
    "From 5\\)\\\n",
    "$x_0 \\in B(x^*, \\epsilon) \\implies \n",
    "\\|x_1 - x^* \\| \\leq c_1 c_2 \\|x_0 - x^*\\|^2 \n",
    "\\leq c_1 c_2 \\left(\\frac{\\alpha}{c_1 c_2}\\right)\\|x_0 - x^*\\| = \\alpha \\|x_0 - x^*\\| \\leq \\|x_0 - x^*\\| \\hspace{5pt}(*)$\n",
    "\n",
    "$(*)\\implies \\|x_1 - x^* \\| \\leq \\frac{\\alpha}{c_1 c_2}$ \n",
    "\n",
    "$(*)\\implies \\|x_1 - x^* \\| \\leq \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bold{7)}$\n",
    "\n",
    "From 5\\),\\\n",
    "$x_k \\in B(x^*, \\epsilon) \\implies \\|x_{k+1} - x^* \\| \\leq c_1 c_2 \\|x_k - x^* \\| ^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bold{8)}$\n",
    "\n",
    "$x_k \\in B(x^*, \\epsilon) \\implies \\|x_{k+1} - x^* \\| \\leq c_1 c_2 \\|x_k - x^* \\| ^2$\n",
    "\n",
    "Using 6\\)\\\n",
    "$x_k \\in B(x^*, \\epsilon) \\land \\|x_k - x^* \\| \\leq \\frac{\\alpha}{c_1 c_2} \\implies x_{k+1} \\in B(x^*, \\epsilon) \\land \\|x_{k+1} - x^* \\| \\leq \\frac{\\alpha}{c_1 c_2}$\n",
    "\n",
    "6\\) is recursive, so $x_0 \\in B(x^*, \\epsilon) \\land \\|x_0 - x^* \\| \\leq \\frac{\\alpha}{c_1 c_2} \\implies \\forall k \\in \\mathbb{N}, \\, x_k \\in B(x^*, \\epsilon) \\land \\|x_k - x^* \\| \\leq \\frac{\\alpha}{c_1 c_2}$\n",
    "\n",
    "Using 7\\)\\\n",
    "$\\forall k \\in \\mathbb{N}, \\, x_k \\in B(x^*, \\epsilon) \\implies \n",
    "\\forall k \\in \\mathbb{N}, \\, \\|x_{k+1} - x^* \\| \\leq c_1 c_2 \\|x_k - x^* \\| ^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Part 3: A deeper dive into neural network implementations [3 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Download datasets\n",
    "train_set_mnist = torchvision.datasets.MNIST(root=parent_dir, download=True,\n",
    "                                         train=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_set_mnist = torchvision.datasets.MNIST(root=parent_dir,download=True,\n",
    "                                        train=False,transform=transforms.Compose([transforms.ToTensor()]),)\n",
    "\n",
    "train_set_cifar = torchvision.datasets.CIFAR10(root=parent_dir, download=True,\n",
    "                                         train=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_set_cifar = torchvision.datasets.CIFAR10(root=parent_dir,download=True,\n",
    "                                        train=False,transform=transforms.Compose([transforms.ToTensor()]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2ab2e42fa10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed\n",
    "SEED = '01877978'\n",
    "np.random.seed(int(SEED))\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Part 3.1: Implementations [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can of course add more cells of both code and markdown. Please remember to comment the code and explain your reasoning. Include docstrings. Tutorial provide a good example of how to style your code.\n",
    "# Although not compulsory you could challenge yourself by using object oriented programming to structure your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, dim, nclass, width, depth):\n",
    "        super().__init__()\n",
    "        self.input_layers = nn.ModuleList()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "        self.act_fn = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(width, nclass)\n",
    "\n",
    "        self.input_layers.append(nn.Flatten())\n",
    "        self.input_layers.append(nn.Linear(dim, width))\n",
    "\n",
    "        for n in range(depth):\n",
    "            self.hidden_layers.append(nn.Linear(width, width))\n",
    "\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = input.float()\n",
    "        for layer in self.input_layers:\n",
    "            x = layer(x)\n",
    "            x = self.act_fn(x)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "            x = self.act_fn(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_data(batch_size, train_set, test_set):\n",
    "        train_dataloader = DataLoader(dataset = train_set, batch_size = batch_size, shuffle = True)\n",
    "        test_dataloader = DataLoader(dataset = test_set, batch_size = batch_size, shuffle = True)\n",
    "        return train_dataloader, test_dataloader \n",
    "    \n",
    "def train_epoch(trainloader, net, optimizer, criterion):\n",
    "    running_loss = 0\n",
    "    for inputs, labels in trainloader:\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "        outputs = net(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Calculate loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(trainloader)\n",
    "\n",
    "def train_test(testloader, net, criterion):\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient tracking for evaluation\n",
    "        for inputs, labels in testloader:\n",
    "            outputs = net(inputs)  # Forward pass\n",
    "            total_loss += criterion(outputs, labels).item()  \n",
    "            predicted = torch.argmax(outputs, dim = 1)\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "            \n",
    "    average_loss = total_loss / len(testloader)\n",
    "    accuracy = correct / len(testloader.dataset)\n",
    "    \n",
    "    return average_loss, 1 - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000 | Train Loss: 0.5101 |Test Loss: 0.2678 | Test Error: 0.0776\n",
      "Epoch: 001 | Train Loss: 0.2352 |Test Loss: 0.1908 | Test Error: 0.0562\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 10\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     test_loss, test_err \u001b[38;5;241m=\u001b[39m train_test(testloader, net, criterion)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.04\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m |Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.04\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Test Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_err\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.04\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(trainloader, net, optimizer, criterion)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m trainloader:\n\u001b[0;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero gradients\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)  \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layers:\n\u001b[1;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(x)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layers:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainloader, testloader = loading_data(64, train_set_mnist, test_set_mnist)\n",
    "        \n",
    "num_epochs = 100\n",
    "net = Net(784, 10, 100, 2)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adamax(net.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(trainloader, net, optimizer, criterion)\n",
    "    test_loss, test_err = train_test(testloader, net, criterion)\n",
    "    print(f'Epoch: {epoch:03} | Train Loss: {train_loss:.04} |Test Loss: {test_loss:.04} | Test Error: {test_err:.04}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Part 3.2: Numerical exploration [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Depth          | Train Loss     | Test Loss       |\n",
    "| -------------- | -------------- | -------------- |\n",
    "| 1              | Row 1, Col 2   | Row 1, Col 3   |\n",
    "| 5              | Row 2, Col 2   | Row 2, Col 3   |\n",
    "| 10             |                | Row 3, Col 3   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Width | Train loss | Test loss |\n",
    "|-------|------------|-----------|\n",
    "| 4     |            |           |\n",
    "| 8     |            |           |\n",
    "| 16    |            |           |\n",
    "| 32    |            |           |\n",
    "| 64    |            |           |\n",
    "| 128   |            |           |\n",
    "| 256   |            |           |\n",
    "| 512   |            |           |\n",
    "| 1024  |            |           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Part 4: The link between Neural Networks and Gaussian Processes [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4.1: Proving the relationship between a Gaussian process and a neural network [4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Proper weight scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "Var(f_i^{(l)}) &=  Var\\left( \\sum_{j=1}^{N_{l-1}} w_{ij}^{(l)}g_j^{(l)}(x) + b_i^{(l)} \\right)\\\\\n",
    "&= Cov\\left(\\sum_{j=1}^{N_{l-1}} w_{ij}^{(l)}g_j^{(l)}(x) + b_i^{(l)}, \\sum_{k=1}^{N_{l-1}} w_{ik}^{(l)}g_k^{(l)}(x) + b_i^{(l)} \\right)\\\\\n",
    "&= \\sigma_b + \\sum_{j=1}^{N_{l-1}} \\sum_{k=1}^{N_{l-1}} Cov \\left( w_{ij}^{(l)}g_j^{(l)}(x),  w_{ik}^{(l)}g_k^{(l)}(x)\\right)\\\\\n",
    "&= \\sigma_b + C_w^{(l)} \\sum_{j=1}^{N_{l-1}} Var(g_j^{(l)}(x))\\\\\n",
    "&= \\sigma_b + N_{l-1} C_w^{(l)} Var(g_j^{(l)}(x))\\\\\n",
    "&= \\sigma_b + N_{l-1} C_w^{(l)} Var(g_j^{(l)}(x)) \\hspace{10pt} (*)\n",
    "\\end{align*}\n",
    "$C_w^{(l)} = \\frac{\\sigma_w}{N_{l-1}} \\implies Var(f_i^{(l)}) = \\frac{1}{\\sigma_b} + \\sigma_w Var(g_j^{(l)}(x))$, constant w.r.t  $N_{l-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Derive the GP relation for a single hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f^{(2)} = W^{(2)}g^{(1)}(x) + b^{(2)}$\n",
    "\n",
    "$f_i^{(2)} =  \\sum_{j=1}^{N_1} w_{ij}^{(2)}g_j^{(1)}(x) + b_i^{(2)}$\\\n",
    "$g_j$ independent $\\forall i$ and $w_{ij}$ independent $\\forall i, j \\, : \\, i \\neq j \\implies$ $w_{ij}g_i$ independent $\\forall i$\\\n",
    "similarly $\\forall i$, $w_{ij}$ is identically distributed\n",
    "\n",
    "By CLT, for large $N_1, \\hspace{28pt} \\frac{1}{\\sqrt{N_1}} \\sum_{j=1}^{N_1} w_{ij}^{(2)}g_j^{(1)}(x) \\hspace{8pt} \\sim N\\left(\\mu, C_w Var[g_j^{(1)}(x)]\\right)$\n",
    "\n",
    "$\\hspace{107pt} \\sum_{j=1}^{N_1} w_{ij}^{(2)}g_j^{(1)}(x) \\hspace{25pt} \\sim N\\left(0, \\sigma_w Var[g_j^{(1)}(x)]\\right)$\n",
    "\n",
    "$\\hspace{78pt} f_i^{(2)} = \\sum_{j=1}^{N_1} w_{ij}^{(2)}g_j^{(1)}(x) + b_i^{(2)}  \\sim N\\left(\\mu, \\sigma_w Var[g_j^{(1)}(x)]  + \\sigma_b \\right)$\n",
    "\n",
    "Let $\\bold{x}$ be the vector of different inputs\n",
    "\n",
    "$f_i^{(2)}(\\bold{x}) =\n",
    "\\sum\\limits_{j=0}^{N_1} w_{ij} \\begin{pmatrix}\n",
    "    g_j(x^{(1)}) \\\\\n",
    "    g_j(x^{(2)}) \\\\\n",
    "    ...   \\\\\n",
    "    g_j(x^{(M)})\n",
    "\\end{pmatrix} =\n",
    "\\sum\\limits_{j=0}^{N_1} G_j(\\bold{x})$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "Cov[f_i^{(2)}(x), f_i^{(2)}(x')] &=  Cov\\left[\\sum_{j=1}^{N_1} w_{ij}^{(2)}g_j^{(1)}(x) + b_i^{(2)},  \\sum_{k=1}^{N_1} w_{ik}^{(2)}g_k^{(1)}(x') + b_i^{(2)}\\right]\\\\\n",
    "&=\\sigma_b + \\sum_{j=1}^{N_1} \\sum_{k=1}^{N_1} Cov\\left[ w_{ij}^{(2)}g_j^{(1)}(x), w_{ik}^{(2)}g_k^{(1)}(x') \\right]\\\\\n",
    "&=\\sigma_b + C_w \\sum_{j=1}^{N_1} Cov\\left[g_j^{(1)}(x), g_j^{(1)}(x')\\right]\\\\\n",
    "&=\\sigma_b + C_w \\sum_{j=1}^{N_1} Cov\\left[\\phi(f^{(1)})(x), \\phi(f^{(1)})(x')\\right]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Why in succession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take limits in succession so now the inputs to the second order are $g(f(x))$, with $f(x)$ distributed normally, (since it tends toward normal in distribution), so we can say that the $f_i$ are independent (uncorrelated $\\implies$ independent for normally distribtuted) and therefore the gs are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Derive the GP relation for multiple hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show $f_j^{(l-1)}$ is a GP $\\implies f_j^{(l)}$ is a GP.\n",
    "\n",
    "$f_j^{(l)}(\\bold{x}) = \n",
    "\\sum\\limits_{j=0}^{N_{l-1}} w_{ij} \\begin{pmatrix}\n",
    "    g_j^{(l-1)}(x^{(1)}) \\\\\n",
    "    g_j^{(l-1)}(x^{(2)}) \\\\\n",
    "    ...   \\\\\n",
    "    g_j^{(l-1)}(x^{(M)}) \n",
    "\\end{pmatrix} + \n",
    "\\begin{pmatrix}\n",
    "    b_i^{(l-1)}(x^{(1)}) \\\\\n",
    "    b_i^{(l-1)}(x^{(2)}) \\\\\n",
    "    ...   \\\\\n",
    "    b_i^{(l-1)}(x^{(M)}) \n",
    "\\end{pmatrix} = \n",
    "\\sum\\limits_{j=0}^{N_1} G_j(\\bold{x}) + B_i^{l-1}(\\bold{x})\n",
    "$\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "Need to show $ \\forall j, G_j(\\bold{x})$ is idependent with finite first and second moments.\n",
    "\n",
    "<!-- $Cov\\left[g_j^{(l-1)}(x), g_k^{(l-1)}(x')\\right] =\n",
    "Cov\\left[\\phi \\left(f_j^{(l-1)}(x)\\right), \\phi \\left(f_k^{(l-1)}(x')\\right)\\right]$ -->\n",
    "\n",
    "Note: If $j \\neq k$ or $x \\neq x', \\, Cov\\left[ f_j^{(l-1)}(x), (f_k^{(l-1)}(x')\\right] = 0$ and $f_j^{(l-1)}(x')$, $f_k^{(l-1)}(x')$ are normally distributed by assumption, so uncorrelated $\\implies$ independent $\\implies \\phi \\left(f_j^{(l-1)}(x)\\right), \\phi \\left(f_k^{(l-1)}(x')\\right)$ independent (as $\\phi$ is a deterministic function)\n",
    "\n",
    "So ,if $j \\neq k$ or $x \\neq x'$, $g_j^{(l-1)}(x)$ and  $g_k^{(l-1)}(x')$ are independent $ \\implies  \\forall j, G_j(\\bold{x})$ independent\n",
    "\n",
    "By CLT, $f_j^{(l)}(\\bold{x})$ is a GP.\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "$E\\left[ f_j^{(l)}(\\bold{x})\\right] = E\\left[ \n",
    "\\sum\\limits_{j=0}^{N_{l-1}} w_{ij} \\begin{pmatrix}\n",
    "    g_j^{(l-1)}(x^{(1)}) \\\\\n",
    "    g_j^{(l-1)}(x^{(2)}) \\\\\n",
    "    ...   \\\\\n",
    "    g_j^{(l-1)}(x^{(M)}) \n",
    "\\end{pmatrix}\n",
    "\\right] = \n",
    "\\sum\\limits_{j=0}^{N_{l-1}} E[w_{ij}] E\\left[ \n",
    "\\begin{pmatrix}\n",
    "    g_j^{(l-1)}(x^{(1)}) \\\\\n",
    "    g_j^{(l-1)}(x^{(2)}) \\\\\n",
    "    ...   \\\\\n",
    "    g_j^{(l-1)}(x^{(M)}) \n",
    "\\end{pmatrix}\n",
    "\\right] = \\bold{0}$\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "$Cov\\left[ f_j^{(l)}(x), f_j^{(l)}(x') \\right] = \n",
    "E\\left[ f_j^{(l)}(x)f_j^{(l)}(x')\\right] =\\\\\n",
    "E\\left[ \\sum\\limits_{j=0}^{N_{l-1}} \\sum\\limits_{k=0}^{N_{l-1}} \\left(w_{ij} g_j^{(l-1)}(x) + b_i(x) \\right) \\left(g_k^{(l-1)}(x') + b_i(x')\\right) \\right] = \\\\\n",
    "\\sum\\limits_{j=0}^{N_{l-1}} \\sum\\limits_{k=0}^{N_{l-1}} E[ w_{ij} w_{ik}] E[g_j^{(l-1)}(x) g_k^{(l-1)}(x') ]  =\\\\\n",
    "C_w \\sum\\limits_{k=0}^{N_{l-1}} E[g_j^{(l-1)}(x) g_k^{(l-1)}(x') ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Part 4.2: Analysing the performance of the Gaussian process and a neural network [4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please use float64 as default dtype for this part of the assignment\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# Another hint: when  computing [ K^L(X,X) + noise^2 Id ]^-1 y and  [ K^L(X,X) + noise^2 Id ]^-1 K^L(X,X*)\n",
    "# You can TRY cholesky solve as it should be p.d. (except case for numerical errors) - maybe you can use try:/except:\n",
    "# You can also try to enforce symmetry in posterior covariance by doing (K + K.t())/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can of course add more cells of both code and markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mapping_dic = {0:-.5,\n",
    "                      1:.5} \n",
    "train_set_cifar.targets = torch.tensor(train_set_cifar.targets, dtype = torch.float64)\n",
    "train_set_cifar.targets = train_set_cifar.targets.apply_(lambda x : target_mapping_dic.get(x,x))\n",
    "\n",
    "#TESTING\n",
    "boolean_mask = (train_set_cifar.targets == 0.5) | (train_set_cifar.targets ==-0.5)\n",
    "train_set_cifar.data = train_set_cifar.data[boolean_mask]\n",
    "train_set_cifar.targets = train_set_cifar.targets[boolean_mask]\n",
    "\n",
    "training_data = torch.tensor(train_set_cifar.data, dtype = torch.float64)[:1000]\n",
    "training_targets = train_set_cifar.targets[:1000]\n",
    "\n",
    "X1 = training_data[:900]\n",
    "X2 = training_data[900:]\n",
    "y1 = training_targets[:900]\n",
    "y2 = training_targets[900:]\n",
    "\n",
    "sigma_b = 1\n",
    "sigma_w = 1\n",
    "\n",
    "X1 = X1.flatten(start_dim=1)\n",
    "X2 = X2.flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "class kernel():\n",
    "    def __init__(self, sigma_b, sigma_w) -> None:\n",
    "        self.sigma_b = sigma_b\n",
    "        self.sigma_w = sigma_w\n",
    "    \n",
    "    #CHANGE THIS\n",
    "    @lru_cache(maxsize=None)\n",
    "    def kernel_diag(self, x, dim):\n",
    "        \"\"\"\n",
    "        For calculating the K(x,x) i.e when x' = x\n",
    "        We can find the diagonal without ever explicitly calculating the matrix \n",
    "        \"\"\"\n",
    "\n",
    "        N0 = x.shape[1]\n",
    "        if dim == 0:\n",
    "            return self.sigma_b + self.sigma_w * torch.norm(x, dim=1) ** 2 / N0\n",
    "        \n",
    "        xx_diag = self.kernel_diag(x, dim -1)\n",
    "        theta = torch.arccos(torch.ones(x.shape[0]))\n",
    "\n",
    "        return self.sigma_b + self.sigma_w / (2 * torch.pi) * xx_diag * (torch.sin(theta) + (torch.pi - theta) * torch.cos(theta))\n",
    "\n",
    "    def theta(self, x1, x2, dim):\n",
    "        K_x_x_diag = self.kernel_diag(x1, dim)\n",
    "        K_xs_x = self.GPKernel(x1, x2, dim)\n",
    "        K_xs_xs_diag = self.kernel_diag(x2, dim)\n",
    "        \n",
    "        return torch.arccos(torch.clamp(K_xs_x / torch.sqrt(torch.outer(K_x_x_diag, K_xs_xs_diag)), min=-1, max=1))\n",
    "        \n",
    "    def GPKernel(self, x1, x2, dim = None):\n",
    "        N0 = x1.shape[1]\n",
    "        if dim == 0:\n",
    "            return self.sigma_b + self.sigma_w * (torch.inner(x1, x2) / N0)\n",
    "        \n",
    "        K_x_x_diag = self.kernel_diag(x1, dim - 1)\n",
    "        K_xs_xs_diag = self.kernel_diag(x2, dim - 1)\n",
    "        theta_val = self.theta(x1, x2, dim -1)\n",
    "\n",
    "        return (self.sigma_b + self.sigma_w / (2 * torch.pi) * torch.sqrt(torch.outer(K_x_x_diag, K_xs_xs_diag)) * \n",
    "                (torch.sin(theta_val) + (torch.pi - theta_val) * torch.cos(theta_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPredictor(kernel):\n",
    "    def __init__(self, sigma_b, sigma_w) -> None:\n",
    "        super().__init__(sigma_b, sigma_w)\n",
    "\n",
    "    def get_variance_and_mean(self, X_train, X_test, y_train, layer):\n",
    "        N = X_train.shape[0]\n",
    "        K_XX = self.GPKernel(X_train,X_train, layer)\n",
    "        K_XsXs = self.GPKernel(X_test, X_test, layer)\n",
    "        K_XsX = self.GPKernel(X_test, X_train, layer)\n",
    "\n",
    "        #CHANGE THIS\n",
    "        sigma = 1\n",
    "\n",
    "        mean_GP = K_XsX @ torch.linalg.inv(K_XX + sigma * torch.eye(N,N)) @ y_train\n",
    "        var_GP = K_XsXs - K_XsX @ torch.linalg.inv(K_XX + sigma * torch.eye(N,N)) @ K_XsX.T\n",
    "\n",
    "        return mean_GP, var_GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(sigma_b, sigma_w, layer, X_train, y_train, X_test, y_test):\n",
    "    GP_predictor = GaussianPredictor(sigma_b, sigma_w)\n",
    "    prediction = torch.sign(GP_predictor.get_variance_and_mean(X_train, X_test, y_train, layer)[0])\n",
    "\n",
    "    return (prediction == torch.sign(y_test)).sum() / len(y_test)\n",
    "\n",
    "def GPgrid_search(sigma_b_list, sigma_w_list, layers, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    results = torch.zeros(len(sigma_b_list), len(sigma_w_list), layers + 1, dtype=torch.float64)\n",
    "    for i, sigma_b in enumerate(sigma_b_list):\n",
    "        for j, sigma_w in enumerate(sigma_w_list):\n",
    "            for l in range(layers + 1):\n",
    "                results[i,j,l] = get_accuracy(sigma_b, sigma_w, l, X_train, y_train, X_test, y_test)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_b_list = np.linspace(0,.8, 20)\n",
    "sigma_w_list = np.linspace(0,3, 20)\n",
    "layers = 10\n",
    "\n",
    "#CHANGE THIS\n",
    "X_train = X1\n",
    "y_train = y1\n",
    "X_test = X2\n",
    "y_test = y2\n",
    "\n",
    "results = GPgrid_search(sigma_b_list, sigma_w_list, layers, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For layer 0, the values that optimize the accuracy sigma_b: 0.04211 | sigma_w :0.1579 | Accuracy : 0.64\n",
      "For layer 1, the values that optimize the accuracy sigma_b: 0.3789 | sigma_w :0.3158 | Accuracy : 0.75\n",
      "For layer 2, the values that optimize the accuracy sigma_b: 0.0 | sigma_w :0.1579 | Accuracy : 0.79\n",
      "For layer 3, the values that optimize the accuracy sigma_b: 0.3368 | sigma_w :0.4737 | Accuracy : 0.83\n",
      "For layer 4, the values that optimize the accuracy sigma_b: 0.3789 | sigma_w :0.7895 | Accuracy : 0.83\n",
      "For layer 5, the values that optimize the accuracy sigma_b: 0.2526 | sigma_w :0.7895 | Accuracy : 0.81\n",
      "For layer 6, the values that optimize the accuracy sigma_b: 0.2105 | sigma_w :0.9474 | Accuracy : 0.81\n",
      "For layer 7, the values that optimize the accuracy sigma_b: 0.2526 | sigma_w :0.9474 | Accuracy : 0.81\n",
      "For layer 8, the values that optimize the accuracy sigma_b: 0.2105 | sigma_w :1.105 | Accuracy : 0.81\n",
      "For layer 9, the values that optimize the accuracy sigma_b: 0.2526 | sigma_w :1.105 | Accuracy : 0.81\n",
      "For layer 10, the values that optimize the accuracy sigma_b: 0.2105 | sigma_w :1.263 | Accuracy : 0.81\n"
     ]
    }
   ],
   "source": [
    "for l in range(layers + 1):\n",
    "    parameter_indices = torch.unravel_index(torch.argmax(results[:,:,l]), results.shape[:2])\n",
    "    sigma_b_index, sigma_w_index = parameter_indices\n",
    "    best_sigma_b = sigma_b_list[sigma_b_index]\n",
    "    best_sigma_w = sigma_w_list[sigma_w_index]\n",
    "    print(f\"For layer {l}, the values that optimize the accuracy sigma_b: {best_sigma_b:.04} | sigma_w :{best_sigma_w:.04} | Accuracy : {results[parameter_indices][l]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_mat = GaussianPredictor(0.2105 , 1.263).get_variance_and_mean(X_train, X_test, y_train, 10)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(35), tensor(65))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most correlated test images (highest covariance)\n",
    "covariance_without_diag = covariance_mat - torch.diag(torch.diag(covariance_mat))\n",
    "index = torch.argmax(covariance_without_diag)\n",
    "torch.unravel_index(index, covariance_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img1 = X_test[35].reshape(32,32,3)\n",
    "img2 = X_test[65].reshape(32,32,3)\n",
    "img1 = img1.type(torch.uint8)\n",
    "img2 = img2.type(torch.uint8)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "axes[0].imshow(img1)\n",
    "axes[1].imshow(img2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(57), tensor(89))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Least correlated test images (lowest covariance)\n",
    "covariance_without_diag = covariance_mat - torch.diag(torch.diag(covariance_mat))\n",
    "index = torch.argmax( - covariance_without_diag)\n",
    "torch.unravel_index(index, covariance_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img1 = X_test[57].reshape(32,32,3)\n",
    "img2 = X_test[89].reshape(32,32,3)\n",
    "img1 = img1.type(torch.uint8)\n",
    "img2 = img2.type(torch.uint8)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "axes[0].imshow(img1)\n",
    "axes[1].imshow(img2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computational cost of NNGP**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
>>>>>>> 5e14e7a (answered some questions)
  },
  "nbformat": 4,
  "nbformat_minor": 0
}