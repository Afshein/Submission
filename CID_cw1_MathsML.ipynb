{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqeaYfHXGAt8"
      },
      "source": [
        "# Coursework 1 - Mathematics for Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjnXLCaKGAt9"
      },
      "source": [
        "## CID: 01877978\n",
        "\n",
        "**Colab link:** insert colab link here\n",
        "\n",
        "***\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d81KQmqIGAt9"
      },
      "source": [
        "## Part 1: Quickfire questions [3 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSUkzJb3GAt9"
      },
      "source": [
        "#### Question 1 (True risk / Empirical risk):\n",
        "\n",
        "True risk $R(f) = \\mathbb{E}_D[L(f(\\bold{x},\\bold{y}))]$\\\n",
        "Empirical risk $\\hat{R}(f) = \\frac{1}{N}\\sum_{i=1}^N L(f(\\bold{x}^i),\\bold{y}^i)$\n",
        "\n",
        "The True risk $R(f)$ can only be evaluated by knowing the joint distrubution of ($\\bold{x}, \\bold{y})$, so is instead approximated by using realisations of the distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suNH_xDLGAt9"
      },
      "source": [
        "#### Question 2 ('Large' or 'rich' hypothesis class):\n",
        "Hypothesis class $F$ is the set of functions which can be used to model $(\\bold{x}, f(\\bold{x}))$\\\n",
        "Downfalls are it is harder to optimize over as you have introduced more parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMx6oUl0GAt9"
      },
      "source": [
        "#### Question 3 (Dataset splitting):\n",
        "\n",
        "No, you chose the model with the maximum validation set score.\n",
        "\n",
        "If all the model validation set score was i.i.d $\\sim N(\\mu, \\sigma^2)$,the maximum would be distributed with $ P(X_{max} \\leq x) = \\prod_{i=1}^{N}[P(X_i \\leq x)] = \\Phi^n(x)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz_reCJlGAt9"
      },
      "source": [
        "#### Question 4 (Occamâ€™s razor):\n",
        "\n",
        "Among competing hypotheses that explain known observations equally well, we should choose the simplest one. In relation to data such as images, this could include"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArGqpktYGAt9"
      },
      "source": [
        "#### Question 5 (Generalisation error):\n",
        "\n",
        "You would expect a small generalisation error on a good model, i.e. it is not overfit on the training set so it performly similarly in training as in testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiDt-rbNGAt-"
      },
      "source": [
        "#### Question 6 (Rademacher complexity pt1):\n",
        "\n",
        "High empirical rademacher complexity means for a given sample $\\{ z_1, z_2, ... z_m\\}$, that our function class does not contain a function that maps the input well to random noise. This means our function class is not too wide that it could fit to data which is unpredictable (random noise)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9CHk3zXGAt-"
      },
      "source": [
        "#### Question 7 (Rademacher complexity pt2):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5APbH5wGAt-"
      },
      "source": [
        "#### Question 8 (Regularisation term in the loss function):\n",
        "\n",
        "Regularisation term in the loss function can encourage the model to adopt a simpler complexity in the training. This can lead to a model with better generalisation ability and which is not overfit to the training data. Also"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHZRJk_0GAt-"
      },
      "source": [
        "#### Question 9 (Momentum gradient descent):\n",
        "\n",
        "Momentum helps the gradient descent escape local minima pits (the momentum accumulated when going in to the local minima helps carry it out). It doesn't take very small steps at gentle gradients, the descent is generally smoother (the gradient of step taken is correlated to gradient of past step taken)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O3G7t7GGAt-"
      },
      "source": [
        "#### Question 10 (Adam):\n",
        "\n",
        "\\begin{align}\n",
        "&t &&= t + 1\\\\\n",
        "&m_t &&= \\beta_1 m_t + (1 - \\beta_1) \\left(\\frac{\\delta L}{\\delta w_t}\\right)\\\\\n",
        "&v_t &&= \\beta_2 v_t + (1 - \\beta_2) \\left(\\frac{\\delta L}{\\delta w_t}\\right)^2\\\\\n",
        "&\\hat{m}_t &&= m_t / (1 - \\beta_1^t)\\\\\n",
        "&\\hat{v}_t &&= v_t / (1 - \\beta_2^t)\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0ejxFM1GAt-"
      },
      "source": [
        "#### Question 11 (AdaGrad):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UXJJoI_GAt-"
      },
      "source": [
        "#### Question 12 (Decaying Learning Rate):\n",
        "\n",
        "Enter your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIqJTm9XGAt-"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Part 2: Short-ish proofs [6 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DoSY99ZGAt-"
      },
      "source": [
        "\n",
        "### Question 2.1: Bounds on the risk [1 point]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeL6-0HxGAt-"
      },
      "source": [
        "Hoeffdings inequality\n",
        "\n",
        "Let $X_1, X_2, X_3,..,X_m$ independent, taking values in $[a_i, b_i]$. With $S_m = \\sum_{i=1}^m X_i$.\\\n",
        "$\\forall \\epsilon > 0$\n",
        "\n",
        "$P(S_n - E[S_n] \\geq \\epsilon) \\leq exp\\left \\{ \\begin{aligned}\n",
        "    -\\frac{2\\epsilon^2}{\\sum_{i=1}^n(b_i - a_i)^2}\n",
        "    \\end{aligned}\n",
        "    \\right\\}$\n",
        "\n",
        "Viewing $L(f(x_i), y_i)$ i.i.d random variables (a function of the join distribution $(\\bold{x}, \\bold{y})$)\\\n",
        "Noting $0 \\leq L(f(x_i), y_i) \\leq 1, \\forall \\omega \\in \\Omega$, we take $b_i = 1, a_i = 0, \\forall i$\\\n",
        "and as they are i.i.d, $\\mathbb{E}[S_n] = \\mathbb{E}[L(f(\\bold{x}), \\bold{x})]$\n",
        "\n",
        " $\\forall \\epsilon$, we have\n",
        "$P(\\frac{1}{N}S_n - \\frac{1}{N} E[S_n] \\geq \\epsilon)  = p(S_n - E[S_n] \\geq N \\epsilon) \\leq exp\\left \\{ \\begin{aligned}\n",
        "    -\\frac{2(N \\epsilon)^2}{\\sum_{i=1}^N1^2}\n",
        "    \\end{aligned}\n",
        "    \\right\\}\n",
        "=exp\\left \\{ \\begin{aligned}\n",
        "    -2 N \\epsilon^2\n",
        "    \\end{aligned}\n",
        "    \\right\\}$\n",
        "\n",
        "\n",
        "3\\) d\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZVxO4y_GAt-"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.2: On semi-definiteness [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCSC6v2nGAt-"
      },
      "source": [
        "$f(y) \\geq f(x) + \\nabla f(x)^T(y-x)$\n",
        "\n",
        "Let $g(t) = f(x+tv)$\n",
        "\n",
        "$g'(t) = v^T \\, \\nabla f(x + vt)$\n",
        "\n",
        "$g''(t) = v^T \\cdot\\nabla^2 f(x + tv) \\cdot v$\n",
        "\n",
        "Showing g is convex.\\\n",
        "$\\forall t_1, t_2 \\in \\mathbb{R},\\alpha \\in (0,1),\\\\\n",
        "g(\\alpha t_1  + (1 - \\alpha)t_2) = f(\\bold{x} + (\\alpha t_1  + (1 - \\alpha)t_2)\\bold{v}) = f(\\alpha [\\bold{x} +  t_1 \\bold{v}] + (1-\\alpha)[ \\bold{x} + t_2 \\bold{v}]) \\leq \\alpha f(\\bold{x} +  t_1 \\bold{v}) + (1-\\alpha) f(\\bold{x} + t_2 \\bold{v})\\\\\n",
        "=\\alpha g(t_1) + (1 - \\alpha) g(t_2)$\n",
        "\n",
        "$g$ is convex $\\implies$ $g''(t) \\geq 0$\n",
        "\n",
        "As our choice of $\\bold{v}$ was arbitrary we have $\\forall \\bold{v} \\in \\mathbb{R^d}$, $0 \\leq g''(t) = \\bold{v} ^T \\, \\nabla ^2 f(x + tv) \\, \\bold{v}$\n",
        "\n",
        "So f is positive semi-definite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4EYvsuCGAt-"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.3: A quick recap of momentum [1 point]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWK98-yGGAt-"
      },
      "source": [
        "\n",
        "$\\bold{1)}$\n",
        "\n",
        "$\\forall i \\in \\{ 1, 2, 3,..,d\\}, \\, Q^T(e_i - x*), $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDGV3f_wGAt-"
      },
      "source": [
        "$\\bold{2)}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS7euK6fGAt_"
      },
      "source": [
        "$\\bold{3)}$\\\n",
        "Assuming it is diagnoalizable, the convergence goes at the rate of the greatest magnitude of eigenvalue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRurGEgpGAt_"
      },
      "source": [
        "$\\bold{4)}$\n",
        "\n",
        "$(\\beta + 1 - \\alpha \\lambda_i)^2 - 4 \\beta < 0 \\Longleftrightarrow $ R has complex eigenvalues, R having complex eigenvalues implies they have fixed magnitude $\\sqrt{\\beta}$ which is the rate of convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMeaP9IcGAt_"
      },
      "source": [
        "***\n",
        "\n",
        "### Question 2.4: Convergence proof [3 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAXuTvKpGAt_"
      },
      "source": [
        "$\\bold{1)}$\n",
        "\n",
        "Iterative step: $x_{k+1} = x_k  - (\\nabla ^2(f(x_k))^{-1} \\nabla f(x_k)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-kerUk8GAt_"
      },
      "source": [
        "$\\bold{2)}$\n",
        "\n",
        "With $f(x) = \\frac{1}{2} x^t Q x + b^T x + c$\n",
        "\n",
        "$\\nabla f(x) = Qx + b$\\\n",
        "$\\nabla^2 f(x) = Q$\n",
        "\n",
        "so $x_{k+1} = X_k - Q^{-1} (Qx_k + b) = x_k - x_k + Q^{-1}b = - Q^{-1}b$\n",
        "\n",
        "$- Q^{-1}b = x$, satisfies $x = x - Q^{-1}(Qx + b)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7dgKPINGAt_"
      },
      "source": [
        "$\\bold{3)}$\\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHhAFz1MGAt_"
      },
      "source": [
        "$\\bold{4)}$\n",
        "\n",
        "$\\| x_1 - x^* \\|$, subbing in the definition of $x_1$\n",
        "\n",
        "$= \\| x_0 - (\\nabla ^2f(x_0))^{-1}(\\nabla f(x_0)) - x^* \\|$\n",
        "\n",
        "$ =\\| (\\nabla ^2f(x_0))^{-1} (\\nabla ^2f(x_0)) \\left[x_0 - (\\nabla ^2f(x_0))^{-1}(\\nabla f(x_0)) - x^* \\right]\\|$\n",
        "\n",
        "$ \\leq \\| (\\nabla ^2f(x_0))^{-1} \\| \\| (\\nabla ^2f(x_0)) (x_0 - x^*) - (\\nabla f(x_0)) \\|\\hspace{10pt}$ Using Lemma 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ3YgWzfGAt_"
      },
      "source": [
        "$\\bold{5)}$\\\n",
        "Choose $\\epsilon > 0 \\, : \\, (\\nabla^2 f(x_0))^{-1}$ exists\n",
        "\n",
        "Taylor series on $\\nabla f(x^*)$ around $x_0$\\\n",
        "$\\exists c \\in (x^*, x_0) \\, : \\, $\\\n",
        "$0 = \\nabla f(x^*) = \\nabla f(x_0) + \\nabla^2 f(x_0)(x^* - x_0) + \\frac{1}{2} (x^* - x_0)^T \\nabla^3 f(c) (x^* - x_0)$\n",
        "\n",
        "$\\hspace{10.5pt} \\nabla ^2f(x_0) (x_0 - x^*) - \\nabla f(x_0)\\\\\n",
        "= \\nabla ^2f(x_0) (x_0 - x^*) - \\nabla f(x_0) + \\nabla f(x^*)\\\\\n",
        "= \\nabla ^2f(x_0) (x_0 - x^*) - \\nabla f(x_0) + \\nabla f(x_0) + \\nabla^2 f(x_0)(x^* - x_0) + \\frac{1}{2} (x^* - x_0)^T \\nabla^3 f(c) (x^* - x_0)\\\\\n",
        "= \\nabla ^2f(x_0) (x_0 - x^*) - \\nabla f(x_0) + \\nabla f(x_0) - \\nabla^2 f(x_0)(x_0 - x^*) + \\frac{1}{2} (x^* - x_0)^T \\nabla^3 f(c) (x^* - x_0)\\\\\n",
        "= \\frac{1}{2} (x^* - x_0)^T \\nabla^3 f(x_0) (x^* - x_0)$\n",
        "\n",
        "$\\implies \\| \\nabla ^2f(x_0) (x_0 - x^*) - \\nabla f(x_0) \\| \\leq \\|(x^* - x_0)^T \\nabla^3 f(c) (x^* - x_0) \\|$\n",
        "\n",
        "$\\| x_1 - x^* \\|\n",
        "\\leq \\| (\\nabla ^2f(x_0))^{-1} \\| \\| (\\nabla ^2f(x_0)) (x_0 - x^*) - (\\nabla f(x_0)) \\|\n",
        "\\leq \\| (\\nabla ^2f(x_0))^{-1} \\| \\| \\nabla^3 f(c) \\| \\| (x_0 - x^*) \\| ^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqrnNl9qGAt_"
      },
      "source": [
        "$\\bold{ 6)}$\n",
        "\n",
        "Given $x_0 \\in B(x^*, \\epsilon)\\,  : \\, \\|x_0 - x^*\\| \\leq \\frac{\\alpha}{c_1 c_2}$\n",
        "\n",
        "Show $x_1 \\in B(x^*, \\epsilon) \\land \\|x_1 - x^* \\| \\leq \\frac{\\alpha}{c_1 c_2}$\n",
        "\n",
        "\n",
        "\n",
        "From 5\\)\\\n",
        "$x_0 \\in B(x^*, \\epsilon) \\implies\n",
        "\\|x_1 - x^* \\| \\leq c_1 c_2 \\|x_0 - x^*\\|^2\n",
        "\\leq c_1 c_2 \\left(\\frac{\\alpha}{c_1 c_2}\\right)\\|x_0 - x^*\\| = \\alpha \\|x_0 - x^*\\| \\leq \\|x_0 - x^*\\| \\hspace{5pt}(*)$\n",
        "\n",
        "$(*)\\implies \\|x_1 - x^* \\| \\leq \\frac{\\alpha}{c_1 c_2}$\n",
        "\n",
        "$(*)\\implies \\|x_1 - x^* \\| \\leq \\epsilon$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAcGVQFLGAt_"
      },
      "source": [
        "$\\bold{7)}$\n",
        "\n",
        "From 5\\),\\\n",
        "$x_k \\in B(x^*, \\epsilon) \\implies \\|x_{k+1} - x^* \\| \\leq c_1 c_2 \\|x_k - x^* \\| ^2$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl5dr7feGAt_"
      },
      "source": [
        "$\\bold{8)}$\n",
        "\n",
        "$x_k \\in B(x^*, \\epsilon) \\implies \\|x_{k+1} - x^* \\| \\leq c_1 c_2 \\|x_k - x^* \\| ^2$\n",
        "\n",
        "Using 6\\)\\\n",
        "$x_k \\in B(x^*, \\epsilon) \\land \\|x_k - x^* \\| \\leq \\frac{\\alpha}{c_1 c_2} \\implies x_{k+1} \\in B(x^*, \\epsilon) \\land \\|x_{k+1} - x^* \\| \\leq \\frac{\\alpha}{c_1 c_2}$\n",
        "\n",
        "6\\) is recursive, so $x_0 \\in B(x^*, \\epsilon) \\land \\|x_0 - x^* \\| \\leq \\frac{\\alpha}{c_1 c_2} \\implies \\forall k \\in \\mathbb{N}, \\, x_k \\in B(x^*, \\epsilon) \\land \\|x_k - x^* \\| \\leq \\frac{\\alpha}{c_1 c_2}$\n",
        "\n",
        "Using 7\\)\\\n",
        "$\\forall k \\in \\mathbb{N}, \\, x_k \\in B(x^*, \\epsilon) \\implies\n",
        "\\forall k \\in \\mathbb{N}, \\, \\|x_{k+1} - x^* \\| \\leq c_1 c_2 \\|x_k - x^* \\| ^2$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BexwMwLGAt_"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Part 3: A deeper dive into neural network implementations [3 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PbtiHBzQGAt_"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qqAmocXUGAuA"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "parent_dir = os.path.dirname(current_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n_biS0LGAuA",
        "outputId": "b3a50388-8eb0-440c-9739-3716783896e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Download datasets\n",
        "train_set_mnist = torchvision.datasets.MNIST(root=parent_dir, download=True,\n",
        "                                         train=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "test_set_mnist = torchvision.datasets.MNIST(root=parent_dir,download=True,\n",
        "                                        train=False,transform=transforms.Compose([transforms.ToTensor()]),)\n",
        "\n",
        "train_set_cifar = torchvision.datasets.CIFAR10(root=parent_dir, download=True,\n",
        "                                         train=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "test_set_cifar = torchvision.datasets.CIFAR10(root=parent_dir,download=True,\n",
        "                                        train=False,transform=transforms.Compose([transforms.ToTensor()]),)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L92Vh3bGAuA",
        "outputId": "2279a616-2922-4c5e-abfa-c843f8d86a20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e07e2b85850>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Set seed\n",
        "SEED = '01877978'\n",
        "np.random.seed(int(SEED))\n",
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DiTu7HrGAuH"
      },
      "source": [
        "***\n",
        "\n",
        "### Part 3.1: Implementations [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXAP1uoRGAuH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gupXrnJoGAuH"
      },
      "outputs": [],
      "source": [
        "# You can of course add more cells of both code and markdown. Please remember to comment the code and explain your reasoning. Include docstrings. Tutorial provide a good example of how to style your code.\n",
        "# Although not compulsory you could challenge yourself by using object oriented programming to structure your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54G4Mlw9GAuH"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, dim, nclass, width, depth):\n",
        "        super().__init__()\n",
        "        self.input_layers = nn.ModuleList()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "\n",
        "        self.act_fn = nn.ReLU()\n",
        "        self.output_layer = nn.Linear(width, nclass)\n",
        "\n",
        "        self.input_layers.append(nn.Flatten())\n",
        "        self.input_layers.append(nn.Linear(dim, width))\n",
        "\n",
        "        for n in range(depth):\n",
        "            self.hidden_layers.append(nn.Linear(width, width))\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input.float()\n",
        "        for layer in self.input_layers:\n",
        "            x = layer(x)\n",
        "            x = self.act_fn(x)\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "            x = self.act_fn(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4l56HwHGAuH"
      },
      "outputs": [],
      "source": [
        "def loading_data(batch_size, train_set, test_set):\n",
        "        train_dataloader = DataLoader(dataset = train_set, batch_size = batch_size, shuffle = True)\n",
        "        test_dataloader = DataLoader(dataset = test_set, batch_size = batch_size, shuffle = True)\n",
        "        return train_dataloader, test_dataloader\n",
        "\n",
        "def train_epoch(trainloader, net, optimizer, criterion):\n",
        "    running_loss = 0\n",
        "    for inputs, labels in trainloader:\n",
        "        optimizer.zero_grad()  # Zero gradients\n",
        "        outputs = net(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Calculate loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return running_loss / len(trainloader)\n",
        "\n",
        "def train_test(testloader, net, criterion):\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking for evaluation\n",
        "        for inputs, labels in testloader:\n",
        "            outputs = net(inputs)  # Forward pass\n",
        "            total_loss += criterion(outputs, labels).item()\n",
        "            predicted = torch.argmax(outputs, dim = 1)\n",
        "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
        "\n",
        "    average_loss = total_loss / len(testloader)\n",
        "    accuracy = correct / len(testloader.dataset)\n",
        "\n",
        "    return average_loss, 1 - accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2liw1HXGAuH",
        "outputId": "2b31a599-340a-427a-d787-411972cd08bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 000 | Train Loss: 1.928 |Test Loss: 1.803 | Test Error: 0.6443\n",
            "Epoch: 001 | Train Loss: 1.737 |Test Loss: 1.672 | Test Error: 0.5995\n",
            "Epoch: 002 | Train Loss: 1.659 |Test Loss: 1.648 | Test Error: 0.5867\n",
            "Epoch: 003 | Train Loss: 1.608 |Test Loss: 1.6 | Test Error: 0.5744\n",
            "Epoch: 004 | Train Loss: 1.569 |Test Loss: 1.55 | Test Error: 0.5481\n",
            "Epoch: 005 | Train Loss: 1.531 |Test Loss: 1.561 | Test Error: 0.5606\n",
            "Epoch: 006 | Train Loss: 1.5 |Test Loss: 1.53 | Test Error: 0.542\n",
            "Epoch: 007 | Train Loss: 1.471 |Test Loss: 1.486 | Test Error: 0.5251\n",
            "Epoch: 008 | Train Loss: 1.449 |Test Loss: 1.471 | Test Error: 0.5208\n",
            "Epoch: 009 | Train Loss: 1.43 |Test Loss: 1.452 | Test Error: 0.5091\n",
            "Epoch: 010 | Train Loss: 1.407 |Test Loss: 1.436 | Test Error: 0.5065\n",
            "Epoch: 011 | Train Loss: 1.394 |Test Loss: 1.439 | Test Error: 0.5083\n",
            "Epoch: 012 | Train Loss: 1.371 |Test Loss: 1.449 | Test Error: 0.5158\n",
            "Epoch: 013 | Train Loss: 1.357 |Test Loss: 1.404 | Test Error: 0.4982\n",
            "Epoch: 014 | Train Loss: 1.345 |Test Loss: 1.424 | Test Error: 0.5045\n",
            "Epoch: 015 | Train Loss: 1.332 |Test Loss: 1.404 | Test Error: 0.4927\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[263], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 10\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     test_loss, test_err \u001b[38;5;241m=\u001b[39m train_test(testloader, net, criterion)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.04\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m |Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.04\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Test Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_err\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.04\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[221], line 13\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(trainloader, net, optimizer, criterion)\u001b[0m\n\u001b[0;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)  \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(trainloader)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adamax.py:119\u001b[0m, in \u001b[0;36mAdamax.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    115\u001b[0m     differentiable \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    117\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, grads, exp_avgs, exp_infs, state_steps)\n\u001b[1;32m--> 119\u001b[0m     \u001b[43madamax\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_infs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforeach\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adamax.py:223\u001b[0m, in \u001b[0;36madamax\u001b[1;34m(params, grads, exp_avgs, exp_infs, state_steps, foreach, maximize, differentiable, has_complex, eps, beta1, beta2, lr, weight_decay)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamax\n\u001b[1;32m--> 223\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_infs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adamax.py:283\u001b[0m, in \u001b[0;36m_single_tensor_adamax\u001b[1;34m(params, grads, exp_avgs, exp_infs, state_steps, eps, beta1, beta2, lr, weight_decay, maximize, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    278\u001b[0m norm_buf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[0;32m    279\u001b[0m     [exp_inf\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39madd_(eps)\u001b[38;5;241m.\u001b[39munsqueeze_(\u001b[38;5;241m0\u001b[39m)], \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    280\u001b[0m )\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m differentiable:\n\u001b[1;32m--> 283\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    285\u001b[0m     exp_inf\u001b[38;5;241m.\u001b[39mcopy_(torch\u001b[38;5;241m.\u001b[39mamax(norm_buf, \u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainloader, testloader = loading_data(64, train_set_cifar, test_set_cifar)\n",
        "\n",
        "num_epochs = 100\n",
        "net = Net(3072, 10, 100, 2)\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adamax(net.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(trainloader, net, optimizer, criterion)\n",
        "    test_loss, test_err = train_test(testloader, net, criterion)\n",
        "    print(f'Epoch: {epoch:03} | Train Loss: {train_loss:.04} |Test Loss: {test_loss:.04} | Test Error: {test_err:.04}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhbpCe3GGAuH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcPD3YpGGAuH"
      },
      "source": [
        "***\n",
        "\n",
        "### Part 3.2: Numerical exploration [2 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqihFiBbGAuH"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwEv-dSNGAuI"
      },
      "source": [
        "| Depth          | Train Loss     | Test Loss       |\n",
        "| -------------- | -------------- | -------------- |\n",
        "| 1              | Row 1, Col 2   | Row 1, Col 3   |\n",
        "| 5              | Row 2, Col 2   | Row 2, Col 3   |\n",
        "| 10             |                | Row 3, Col 3   |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSvlCWm5GAuI"
      },
      "source": [
        "| Width | Train loss | Test loss |\n",
        "|-------|------------|-----------|\n",
        "| 4     |            |           |\n",
        "| 8     |            |           |\n",
        "| 16    |            |           |\n",
        "| 32    |            |           |\n",
        "| 64    |            |           |\n",
        "| 128   |            |           |\n",
        "| 256   |            |           |\n",
        "| 512   |            |           |\n",
        "| 1024  |            |           |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0lFXqBmGAuI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnK7kyJrGAuI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY_HKpxHGAuI"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "## Part 4: The link between Neural Networks and Gaussian Processes [8 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2ORgXKaGAuI"
      },
      "source": [
        "### Part 4.1: Proving the relationship between a Gaussian process and a neural network [4 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s-jnqQdGAuI"
      },
      "source": [
        "### Task 1: Proper weight scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3IDqP2VGAuI"
      },
      "source": [
        "\\begin{align*}\n",
        "Var(f_i^{(l)}) &=  Var\\left( \\sum_{j=1}^{N_{l-1}} w_{ij}^{(l)}g_j^{(l)}(x) + b_i^{(l)} \\right)\\\\\n",
        "&= Cov\\left(\\sum_{j=1}^{N_{l-1}} w_{ij}^{(l)}g_j^{(l)}(x) + b_i^{(l)}, \\sum_{k=1}^{N_{l-1}} w_{ik}^{(l)}g_k^{(l)}(x) + b_i^{(l)} \\right)\\\\\n",
        "&= \\sigma_b + \\sum_{j=1}^{N_{l-1}} \\sum_{k=1}^{N_{l-1}} Cov \\left( w_{ij}^{(l)}g_j^{(l)}(x),  w_{ik}^{(l)}g_k^{(l)}(x)\\right)\\\\\n",
        "&= \\sigma_b + C_w^{(l)} \\sum_{j=1}^{N_{l-1}} Var(g_j^{(l)}(x))\\\\\n",
        "&= \\sigma_b + N_{l-1} C_w^{(l)} Var(g_j^{(l)}(x))\\\\\n",
        "&= \\sigma_b + N_{l-1} C_w^{(l)} Var(g_j^{(l)}(x)) \\hspace{10pt} (*)\n",
        "\\end{align*}\n",
        "$C_w^{(l)} = \\frac{\\sigma_w}{N_{l-1}} \\implies Var(f_i^{(l)}) = \\frac{1}{\\sigma_b} + \\sigma_w Var(g_j^{(l)}(x))$, constant w.r.t  $N_{l-1}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycjOg0QXGAuI"
      },
      "source": [
        "### Task 2: Derive the GP relation for a single hidden layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2wA8JTpGAuI"
      },
      "source": [
        "$f^{(2)} = W^{(2)}g^{(1)}(x) + b^{(2)}$\n",
        "\n",
        "$f_i^{(2)} =  \\sum_{j=1}^{N_1} w_{ij}^{(2)}g_j^{(1)}(x) + b_i^{(2)}$\\\n",
        "$g_i$ independent $\\forall i$ and $w_{ij}$ independent $\\forall i, j \\, : \\, i \\neq j \\implies$ $w_{ij}g_i$ independent $\\forall i$\\\n",
        "similarly $\\forall i$, $w_{ij}$ is identically distributed\n",
        "\n",
        "By CLT, for large $N_1, \\hspace{28pt} \\frac{1}{\\sqrt{N_1}} \\sum_{j=1}^{N_1} w_{ij}^{(2)}g_j^{(1)}(x) \\hspace{8pt} \\sim N\\left(\\mu, C_w Var[g_j^{(1)}(x)]\\right)$\n",
        "\n",
        "$\\hspace{107pt} \\sum_{j=1}^{N_1} w_{ij}^{(2)}g_j^{(1)}(x) \\hspace{25pt} \\sim N\\left(0, \\sigma_w Var[g_j^{(1)}(x)]\\right)$\n",
        "\n",
        "$\\hspace{78pt} f_i^{(2)} = \\sum_{j=1}^{N_1} w_{ij}^{(2)}g_j^{(1)}(x) + b_i^{(2)}  \\sim N\\left(\\mu, \\sigma_w Var[g_j^{(1)}(x)]  + \\sigma_b \\right)$\n",
        "\n",
        "Let $\\bold{x}$ be the vector of different inputs\n",
        "\n",
        "$f_i^{(2)}(\\bold{x}) = \\begin{pmatrix}\n",
        "    e_i^T \\\\\n",
        "    e_i^T \\\\\n",
        "    ...   \\\\\n",
        "    e_i^T\n",
        "\\end{pmatrix}\n",
        "W\n",
        "g_i^{(1)}(\\bold{x})\n",
        "= \\sum\\limits_{j=0}^{N_1} w_{ij} \\begin{pmatrix}\n",
        "    g_i(x^{(1)}) \\\\\n",
        "    g_i(x^{(2)}) \\\\\n",
        "    ...   \\\\\n",
        "    g_i(x^{(M)})\n",
        "\\end{pmatrix}\n",
        "= \\sum\\limits_{j=0}^{N_1} G_j(\\bold{x})$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEsztD74GAuI"
      },
      "source": [
        "\\begin{align*}\n",
        "Cov[f_i^{(2)}(x), f_i^{(2)}(x')] &=  Cov\\left[\\sum_{j=1}^{N_1} w_{ij}^{(2)}g_j^{(1)}(x) + b_i^{(2)},  \\sum_{k=1}^{N_1} w_{ik}^{(2)}g_k^{(1)}(x') + b_i^{(2)}\\right]\\\\\n",
        "&=\\sigma_b + \\sum_{j=1}^{N_1} \\sum_{k=1}^{N_1} Cov\\left[ w_{ij}^{(2)}g_j^{(1)}(x), w_{ik}^{(2)}g_k^{(1)}(x') \\right]\\\\\n",
        "&=\\sigma_b + C_w \\sum_{j=1}^{N_1} Cov\\left[g_j^{(1)}(x), g_j^{(1)}(x')\\right]\\\\\n",
        "&=\\sigma_b + C_w \\sum_{j=1}^{N_1} Cov\\left[\\phi(f^{(1)})(x), \\phi(f^{(1)})(x')\\right]\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAfStspiGAuI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9rPuCDQGAuI"
      },
      "source": [
        "### Task 3: Why in succession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpAOg92GGAuI"
      },
      "source": [
        "We take limits in succession so now the inputs to the second order are $g(f(x))$, with $f(x)$ distributed normally, (since it tends toward normal in distribution), so we can say that the $f_i$ are independent (uncorrelated $\\implies$ independent for normally distribtuted) and therefore the gs are independent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b-ksAQtGAuI"
      },
      "source": [
        "### Task 4: Derive the GP relation for multiple hidden layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SS6-tkuGAuI"
      },
      "source": [
        "Show $f_j^{(l-1)}$ is a GP $\\implies f_j^{(l)}$ is a GP.\n",
        "\n",
        "$f_j^{(l)}(\\bold{x}) =\n",
        "\\sum\\limits_{j=0}^{N_{l-1}} w_{ij} \\begin{pmatrix}\n",
        "    g_i^{(l-1)}(x^{(1)}) \\\\\n",
        "    g_i^{(l-1)}(x^{(2)}) \\\\\n",
        "    ...   \\\\\n",
        "    g_i^{(l-1)}(x^{(M)})\n",
        "\\end{pmatrix}\n",
        "$\n",
        "\n",
        "By CLT, $f_j^{(l)}(\\bold{x})$ is a GP.\n",
        "\n",
        "$E\\left[ f_j^{(l)}(\\bold{x})\\right] = E\\left[\n",
        "\\sum\\limits_{j=0}^{N_{l-1}} w_{ij} \\begin{pmatrix}\n",
        "    g_j^{(l-1)}(x^{(1)}) \\\\\n",
        "    g_j^{(l-1)}(x^{(2)}) \\\\\n",
        "    ...   \\\\\n",
        "    g_j^{(l-1)}(x^{(M)})\n",
        "\\end{pmatrix}\n",
        "\\right] =\n",
        "\\sum\\limits_{j=0}^{N_{l-1}} E[w_{ij}] E\\left[\n",
        "\\begin{pmatrix}\n",
        "    g_j^{(l-1)}(x^{(1)}) \\\\\n",
        "    g_j^{(l-1)}(x^{(2)}) \\\\\n",
        "    ...   \\\\\n",
        "    g_j^{(l-1)}(x^{(M)})\n",
        "\\end{pmatrix}\n",
        "\\right] = \\bold{0}$\n",
        "\n",
        "$Cov\\left[ f_j^{(l)}(x), f_j^{(l)}(x') \\right] =\n",
        "E\\left[ f_j^{(l)}(x)f_j^{(l)}(x')\\right] =\\\\\n",
        "E\\left[ \\sum\\limits_{j=0}^{N_{l-1}} \\sum\\limits_{k=0}^{N_{l-1}} \\left(w_{ij} g_j^{(l-1)}(x) + b_i(x) \\right) \\left(g_k^{(l-1)}(x') + b_i(x')\\right) \\right] = \\\\\n",
        "\\sum\\limits_{j=0}^{N_{l-1}} \\sum\\limits_{k=0}^{N_{l-1}} E[ w_{ij} w_{ik}] E[g_j^{(l-1)}(x) g_k^{(l-1)}(x') ]  =\\\\\n",
        "C_w \\sum\\limits_{k=0}^{N_{l-1}} E[g_j^{(l-1)}(x) g_k^{(l-1)}(x') ]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjVF_zA4GAuJ"
      },
      "source": [
        "***\n",
        "\n",
        "### Part 4.2: Analysing the performance of the Gaussian process and a neural network [4 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x4fQUFHGAuJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "hop02aZ5GAuJ"
      },
      "outputs": [],
      "source": [
        "# Please use float64 as default dtype for this part of the assignment\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "# Another hint: when  computing [ K^L(X,X) + noise^2 Id ]^-1 y and  [ K^L(X,X) + noise^2 Id ]^-1 K^L(X,X*)\n",
        "# You can TRY cholesky solve as it should be p.d. (except case for numerical errors) - maybe you can use try:/except:\n",
        "# You can also try to enforce symmetry in posterior covariance by doing (K + K.t())/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqbEiXORGAuJ"
      },
      "outputs": [],
      "source": [
        "# You can of course add more cells of both code and markdown."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_cifar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVD75h-QJMxH",
        "outputId": "7ce24457-9450-462e-a4db-6d5babed0bc5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 50000\n",
              "    Root location: /\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "lCd1rpVMGAuJ"
      },
      "outputs": [],
      "source": [
        "target_mapping_dic = {0:-.5,\n",
        "                      1:.5}\n",
        "target_mapping = lambda x : target_mapping_dic.get(x,x)\n",
        "target_mapping = np.vectorize (target_mapping)\n",
        "train_set_cifar.targets = torch.tensor(train_set_cifar.targets, dtype = torch.float64)\n",
        "train_set_cifar.targets = target_mapping(train_set_cifar.targets)\n",
        "\n",
        "boolean_mask = (train_set_cifar.targets == 0.5) | (train_set_cifar.targets ==-0.5)\n",
        "train_set_cifar.data = train_set_cifar.data[boolean_mask]\n",
        "train_set_cifar.targets = [boolean_mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nasn96REGAuJ"
      },
      "outputs": [],
      "source": [
        "sigma_b = 1\n",
        "sigma_w = 1\n",
        "\n",
        "def theta(x1, x2 ,l):\n",
        "    K_x_x = kernel(x1, x1, l, sigma_b, sigma_w)\n",
        "    K_xs_x = kernel(x1, x2, l, sigma_b, sigma_w)\n",
        "    K_xs_xs = kernel(x2, x2, l, sigma_b, sigma_w)\n",
        "\n",
        "    return torch.arccos(K_xs_x / torch.sqrt(K_x_x * K_xs_xs))\n",
        "\n",
        "def kernel(x1, x2, l, sigma_b, sigma_w):\n",
        "    \"\"\"\n",
        "    This function should take as input the number of layers L, Ïƒ2\n",
        "    w, Ïƒ2, b and two datasets X1, X2 of size M1 Ã— N0 and M2 Ã— N0 where\n",
        "    N0 is, as before, the dimension of the input and M1 and M2 are\n",
        "    the number of datapoints in the first and second dataset, respectively.\n",
        "    The output from this function should be a matrix KL where each element KL_ij\n",
        "    is the kernel function applied to element i from X1 and element j from X2\n",
        ".\n",
        "    \"\"\"\n",
        "    N0 = x1.shape[1]\n",
        "    if l == 0:\n",
        "        return sigma_b + sigma_w *(torch.matmul(x1, x2) / N0)\n",
        "\n",
        "    K_x_x = kernel(x1, x1, l-1, sigma_b, sigma_w)\n",
        "    K_xs_xs = kernel(x2, x2, l-1, sigma_b, sigma_w)\n",
        "\n",
        "    return (sigma_b + sigma_w / (2 * torch.pi) * torch.sqrt(K_x_x * K_xs_xs) *\n",
        "    torch.sin(theta(x1, x2 ,l-1)) + (torch.pi - theta(x1, x2 ,l-1)) * torch.cos(theta(x1, x2 ,l-1)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def kernel_vectorized(x1, x2, l, sigma_b, sigma_w):\n",
        "    N0 = x1.shape[1]\n",
        "    if l == 0:\n",
        "        return sigma_b + sigma_w *(torch.inner(x1, x2) / N0)\n",
        "\n",
        "    K_x_x = kernel_vectorized(x1, x1, l-1, sigma_b, sigma_w)\n",
        "    K_xs_xs = kernel_vectorized(x2, x2, l-1, sigma_b, sigma_w)\n",
        "\n",
        "    x_x_diag = torch.diag(K_x_x)\n",
        "    xs_xs_diag = torch.diag(K_xs_xs)\n",
        "\n",
        "    return (sigma_b + sigma_w / (2 * torch.pi) * torch.sqrt(torch.outer(x_x_diag, xs_xs_diag)) *\n",
        "    torch.sin(theta(x1, x2 ,l-1)) + (torch.pi - theta(x1, x2 ,l-1)) * torch.cos(theta(x1, x2 ,l-1)))"
      ],
      "metadata": {
        "id": "aSC5lb-XGdXr"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = torch.tensor(train_set_cifar.data)[:1000]\n",
        "training_targets = train_set_cifar.targets[:1000]"
      ],
      "metadata": {
        "id": "AAHCgHwiIhIn"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Jb9C8bzuGAuJ"
      },
      "outputs": [],
      "source": [
        "#TESTING\n",
        "X1 = training_data[:100]\n",
        "X2 = training_data[100:130]\n",
        "\n",
        "X1 = X1.flatten(start_dim = 1)\n",
        "X2 = X2.flatten(start_dim = 1)\n",
        "sigma_b = 1\n",
        "sigma_w = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "njyxEHpHGAuJ",
        "outputId": "0d0ea7b8-7c23-4084-930e-be54bafac4d2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RecursionError",
          "evalue": "maximum recursion depth exceeded in comparison",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-c32ddac1debb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkernel_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-76-885ed240a9b3>\u001b[0m in \u001b[0;36mkernel_vectorized\u001b[0;34m(x1, x2, l, sigma_b, sigma_w)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msigma_b\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigma_w\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mK_x_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mK_xs_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-885ed240a9b3>\u001b[0m in \u001b[0;36mkernel_vectorized\u001b[0;34m(x1, x2, l, sigma_b, sigma_w)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msigma_b\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigma_w\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mK_x_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mK_xs_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-885ed240a9b3>\u001b[0m in \u001b[0;36mkernel_vectorized\u001b[0;34m(x1, x2, l, sigma_b, sigma_w)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     return (sigma_b + sigma_w / (2 * torch.pi) * torch.sqrt(torch.outer(x_x_diag, xs_xs_diag)) * \n\u001b[0;32m---> 13\u001b[0;31m     torch.sin(theta(x1, x2 ,l-1)) + (torch.pi - theta(x1, x2 ,l-1)) * torch.cos(theta(x1, x2 ,l-1)))\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-3f36e96f418c>\u001b[0m in \u001b[0;36mtheta\u001b[0;34m(x1, x2, l)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mK_x_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mK_xs_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mK_xs_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-3f36e96f418c>\u001b[0m in \u001b[0;36mkernel\u001b[0;34m(x1, x2, l, sigma_b, sigma_w)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msigma_b\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigma_w\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mK_x_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mK_xs_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "... last 1 frames repeated, from the frame below ...\n",
            "\u001b[0;32m<ipython-input-10-3f36e96f418c>\u001b[0m in \u001b[0;36mkernel\u001b[0;34m(x1, x2, l, sigma_b, sigma_w)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msigma_b\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigma_w\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mK_x_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mK_xs_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded in comparison"
          ]
        }
      ],
      "source": [
        "kernel_vectorized(X1, X2, 3, sigma_b, sigma_w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88tufQ9HGAuJ",
        "outputId": "8880d391-c29d-48cf-d427-961f35073995"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0238, 1.0195, 1.0589,  ..., 1.0788, 1.0716, 1.0762],\n",
              "        [1.0189, 1.0605, 1.0322,  ..., 1.0570, 1.0459, 1.0531],\n",
              "        [1.0410, 1.0010, 1.0309,  ..., 1.0228, 1.0150, 1.0186],\n",
              "        ...,\n",
              "        [1.0133, 1.0254, 1.0091,  ..., 1.0684, 1.0000, 1.0072],\n",
              "        [1.0599, 1.0182, 1.0143,  ..., 1.0729, 1.0638, 1.0420],\n",
              "        [1.0208, 1.0016, 1.0127,  ..., 1.0583, 1.0433, 1.0303]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "sigma_b + sigma_w *(torch.inner(X1, X2) / X1.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqZzMeeNGAuK",
        "outputId": "0e45eed2-d97d-43a5-d2f3-2953146abb34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[0.2314, 0.1686, 0.1961,  ..., 0.6196, 0.5961, 0.5804],\n",
              "          [0.0627, 0.0000, 0.0706,  ..., 0.4824, 0.4667, 0.4784],\n",
              "          [0.0980, 0.0627, 0.1922,  ..., 0.4627, 0.4706, 0.4275],\n",
              "          ...,\n",
              "          [0.8157, 0.7882, 0.7765,  ..., 0.6275, 0.2196, 0.2078],\n",
              "          [0.7059, 0.6784, 0.7294,  ..., 0.7216, 0.3804, 0.3255],\n",
              "          [0.6941, 0.6588, 0.7020,  ..., 0.8471, 0.5922, 0.4824]],\n",
              " \n",
              "         [[0.2431, 0.1804, 0.1882,  ..., 0.5176, 0.4902, 0.4863],\n",
              "          [0.0784, 0.0000, 0.0314,  ..., 0.3451, 0.3255, 0.3412],\n",
              "          [0.0941, 0.0275, 0.1059,  ..., 0.3294, 0.3294, 0.2863],\n",
              "          ...,\n",
              "          [0.6667, 0.6000, 0.6314,  ..., 0.5216, 0.1216, 0.1333],\n",
              "          [0.5451, 0.4824, 0.5647,  ..., 0.5804, 0.2431, 0.2078],\n",
              "          [0.5647, 0.5059, 0.5569,  ..., 0.7216, 0.4627, 0.3608]],\n",
              " \n",
              "         [[0.2471, 0.1765, 0.1686,  ..., 0.4235, 0.4000, 0.4039],\n",
              "          [0.0784, 0.0000, 0.0000,  ..., 0.2157, 0.1961, 0.2235],\n",
              "          [0.0824, 0.0000, 0.0314,  ..., 0.1961, 0.1961, 0.1647],\n",
              "          ...,\n",
              "          [0.3765, 0.1333, 0.1020,  ..., 0.2745, 0.0275, 0.0784],\n",
              "          [0.3765, 0.1647, 0.1176,  ..., 0.3686, 0.1333, 0.1333],\n",
              "          [0.4549, 0.3686, 0.3412,  ..., 0.5490, 0.3294, 0.2824]]]),\n",
              " 6.0)"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_set_cifar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Kqe-Yi6GAuK",
        "outputId": "ffecc7cf-25e9-42c9-c24b-f6aa70a22f72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function torch._VariableFunctionsClass.arccos>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.arccos"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}